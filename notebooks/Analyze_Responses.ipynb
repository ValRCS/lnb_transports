{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Responses of LLM to prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2025-02-28 17:50:54.356760\n",
      "Python version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "Requests version: 2.32.3\n",
      "Pandas version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "# first we need to import the basic libraries\n",
    "# date\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(f\"Date: {now}\")\n",
    "# python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "from pathlib import Path\n",
    "import json\n",
    "# import time for delay\n",
    "import time\n",
    "import requests\n",
    "# print version\n",
    "print(f\"Requests version: {requests.__version__}\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder: ..\\data\\responses\n",
      "Data folder exists: True\n",
      "Data folder is dir: True\n",
      "Subfolders:\n",
      "..\\data\\responses\\2025_01_28_gemini_2_experimental\n",
      "..\\data\\responses\\2025_01_29_google_gemini-flash-1.5-8b_no_terms\n",
      "..\\data\\responses\\2025_01_29_google_gemini-flash-1.5-8b_with_terms\n",
      "..\\data\\responses\\2025_02_04_google_gemini-flash-1.5-8b_with_terms\n",
      "..\\data\\responses\\2025_02_26_google_gemini-flash-1.5_land_prompt_1\n",
      "..\\data\\responses\\2025_02_26_google_gemini-flash-1.5_land_prompt_2\n",
      "..\\data\\responses\\2025_02_27_google_gemini-2.0-flash-001_land_prompt\n",
      "..\\data\\responses\\2025_02_27_google_gemini-2.0-flash-001_land_prompt_2\n",
      "..\\data\\responses\\2025_02_27_google_gemini-flash-1.5_maritime_prompt\n",
      "..\\data\\responses\\2025_02_28_google_gemini-flash-1.5_air_prompt\n",
      "..\\data\\responses\\consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt\n",
      "..\\data\\responses\\consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2\n",
      "..\\data\\responses\\temp_responses_2025_02_26\n",
      "..\\data\\responses\\unconsolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt\n",
      "..\\data\\responses\\unconsolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2\n"
     ]
    }
   ],
   "source": [
    "# let's see what folders are in our ../data/responses folder\n",
    "data_folder = Path(\"../data/responses\")\n",
    "print(f\"Data folder: {data_folder}\")\n",
    "print(f\"Data folder exists: {data_folder.exists()}\")\n",
    "print(f\"Data folder is dir: {data_folder.is_dir()}\")\n",
    "# let's see what subfolders are in our data folder\n",
    "subfolders = [f for f in data_folder.iterdir() if f.is_dir()]\n",
    "print(f\"Subfolders:\")\n",
    "for subfolder in subfolders:\n",
    "    print(subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate openai responses\n",
    "\n",
    "OpenAI prompts required us to break down files into smaller chunks, now we need to consolidate them back into a single file.\n",
    "\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI folders:\n",
      "..\\data\\responses\\2025_02_26_openai_gpt-4o-2024-11-20_land_prompt\n",
      "..\\data\\responses\\2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2\n"
     ]
    }
   ],
   "source": [
    "# subfolders that contain openai in their name\n",
    "openai_folders = [f for f in data_folder.iterdir() if f.is_dir() and \"openai\" in f.name]\n",
    "print(f\"OpenAI folders:\")\n",
    "for openai_folder in openai_folders:\n",
    "    print(openai_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI files:\n",
      "AustA_KaspG_948026: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/AustA_KaspG_948026_0.txt')]\n",
      "AustA_Puisk_1047362: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/AustA_Puisk_1047362_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/AustA_Puisk_1047362_1.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/AustA_Puisk_1047362_2.txt')]\n",
      "FimbK_KadNa_1049450: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/FimbK_KadNa_1049450_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/FimbK_KadNa_1049450_1.txt')]\n",
      "FimbK_TiltP_1049479: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/FimbK_TiltP_1049479_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/FimbK_TiltP_1049479_1.txt')]\n",
      "GulbA_Gaidi_1350352: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/GulbA_Gaidi_1350352_0.txt')]\n",
      "GulbA_JaunV_1053680: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/GulbA_JaunV_1053680_0.txt')]\n",
      "LaciV_AtbrZ_1051755: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LaciV_AtbrZ_1051755_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LaciV_AtbrZ_1051755_1.txt')]\n",
      "LaciV_PulaE_1053689: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LaciV_PulaE_1053689_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LaciV_PulaE_1053689_1.txt')]\n",
      "LapiK_DodaU_1046848: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LapiK_DodaU_1046848_0.txt')]\n",
      "LapiK_ManaD_1051717: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LapiK_ManaD_1051717_0.txt')]\n",
      "LesiVi_LiktR_1051787: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LesiVi_LiktR_1051787_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LesiVi_LiktR_1051787_1.txt')]\n",
      "LesiVi_Uzvar_1350343: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LesiVi_Uzvar_1350343_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LesiVi_Uzvar_1350343_1.txt')]\n",
      "NiedAi_CilvA_1025449: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/NiedAi_CilvA_1025449_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/NiedAi_CilvA_1025449_1.txt')]\n",
      "NiedAi_Salna_1049440: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/NiedAi_Salna_1049440_0.txt')]\n",
      "PaulM_SirdP_1049495: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/PaulM_SirdP_1049495_0.txt')]\n",
      "PaulM_VienV_1049499: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/PaulM_VienV_1049499_0.txt')]\n",
      "RoziP_DivaS_1053490: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/RoziP_DivaS_1053490_0.txt')]\n",
      "RoziP_UgunC_1046826: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/RoziP_UgunC_1046826_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/RoziP_UgunC_1046826_1.txt')]\n",
      "SartJ_Pagar_1047279: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/SartJ_Pagar_1047279_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/SartJ_Pagar_1047279_1.txt')]\n",
      "SartJ_Zieda_1051763: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/SartJ_Zieda_1051763_0.txt')]\n"
     ]
    }
   ],
   "source": [
    "# we want to create a function that given a subfolder will return a dictionary \n",
    "# keys will be first three parts of file name when split by _\n",
    "# values will be actual file names\n",
    "def get_files(subfolder):\n",
    "    files = {}\n",
    "    for file in subfolder.iterdir():\n",
    "        if file.is_file():\n",
    "            parts = file.name.split(\"_\")\n",
    "            key = \"_\".join(parts[:3])\n",
    "            if key in files:\n",
    "                files[key].append(file)\n",
    "            else:\n",
    "                files[key] = [file]\n",
    "    return files\n",
    "\n",
    "# let's run this function on one of the openai folders\n",
    "openai_files = get_files(openai_folders[0])\n",
    "print(f\"OpenAI files:\")\n",
    "for key, value in openai_files.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025_02_26_openai_gpt-4o-2024-11-20_land_prompt: 20\n",
      "2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2: 20\n"
     ]
    }
   ],
   "source": [
    "# let's run get_files on all openai folders\n",
    "# the key will be folder name and values will be dictionaries returned by get_files\n",
    "openai_files = {}\n",
    "for openai_folder in openai_folders:\n",
    "    openai_files[openai_folder.name] = get_files(openai_folder)\n",
    "\n",
    "# how many files are in each folder\n",
    "for key, value in openai_files.items():\n",
    "    print(f\"{key}: {len(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a function that given a file name and file list and new_subfolder will write consolidated file to new_subfolder\n",
    "# logic is as follows:\n",
    "# we want to read all content of files in file list up to empty line\n",
    "# we want to write all this content to new file in new_subfolder\n",
    "# then we want to separately read all lines starting with line that starts with \"System prompt:\"\n",
    "# we want to write this content only once to new file in new_subfolder\n",
    "# we want use utf-8 encoding\n",
    "def consolidate_files(file_name, file_list, new_subfolder):\n",
    "    # create new subfolder if it does not exist\n",
    "    new_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "    with open(new_subfolder / f\"{file_name}.txt\", \"w\", encoding=\"utf-8\") as new_file:\n",
    "        system_prompts = []\n",
    "        for file in file_list:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as old_file:\n",
    "                text = old_file.read()\n",
    "                # let's split on \"System prompt:\"\n",
    "                parts = text.split(\"System prompt:\")\n",
    "                # let's write first part\n",
    "                new_file.write(parts[0].strip()+\"\\n\")\n",
    "                # append second part to system_prompts\n",
    "                system_prompts.append(parts[1])\n",
    "        # let's write system prompts only once\n",
    "        # first check if system prompts are identical\n",
    "        if len(set(system_prompts)) == 1:\n",
    "            new_file.write(\"\\nSystem prompt:\" + system_prompts[0])\n",
    "        else:\n",
    "            for system_prompt in system_prompts:\n",
    "                new_file.write(\"System prompt:\\n\" + system_prompt)\n",
    "\n",
    "# test it on second key of openai_files\n",
    "# we will create a new subfolder in the data respones folder\n",
    "# new_subfolder = data_folder / \"consolidated\"\n",
    "# consolidate_files(list(openai_files.keys())[1], openai_files[list(openai_files.keys())[1]], new_subfolder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a function that will consolidate all files in all openai folders\n",
    "# we will use consolidate_files function\n",
    "# new subfolder will be in data folder\n",
    "# it will be called consolidated_ + key of openai_files\n",
    "def consolidate_all_files(openai_files, data_folder):\n",
    "    for key, value in openai_files.items():\n",
    "        new_subfolder = data_folder / (\"consolidated_\" + key)\n",
    "        # value is a dictionary that contains keys that are first three parts of file name and values that are lists of files\n",
    "        for key2, value2 in value.items():\n",
    "            consolidate_files(key2, value2, new_subfolder)\n",
    "\n",
    "# let's run this function\n",
    "consolidate_all_files(openai_files, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the subfolders for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders to analyze:\n",
      "..\\data\\responses\\consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt\n",
      "..\\data\\responses\\consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2\n",
      "..\\data\\responses\\2025_02_26_google_gemini-flash-1.5_land_prompt_1\n",
      "..\\data\\responses\\2025_02_26_google_gemini-flash-1.5_land_prompt_2\n",
      "..\\data\\responses\\2025_02_27_google_gemini-2.0-flash-001_land_prompt\n",
      "..\\data\\responses\\2025_02_27_google_gemini-2.0-flash-001_land_prompt_2\n"
     ]
    }
   ],
   "source": [
    "# now we want to get all folders that we want to analyze\n",
    "# they are in data_folder \n",
    "# we want those that start with consolidated_2025_02_26 or consolidated_2025_02_27\n",
    "# we also want those that start with 2025_02_26 or 2025_02_27 and also contain words land_prompt\n",
    "# these will be the folders that we want to analyze\n",
    "folders_to_analyze = [f for f in data_folder.iterdir() if f.is_dir() and (f.name.startswith(\"consolidated_2025_02_26\") or f.name.startswith(\"consolidated_2025_02_27\"))]\n",
    "folders_to_analyze += [f for f in data_folder.iterdir() if f.is_dir() and (f.name.startswith(\"2025_02_26\") or f.name.startswith(\"2025_02_27\")) and \"land_prompt\" in f.name]\n",
    "print(f\"Folders to analyze:\")\n",
    "for folder in folders_to_analyze:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Plaintext into  memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [00:07<00:00, 61.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 458\n",
      "Total characters: 191069647\n",
      "Key for smallest text: VentA_DepuT_1293527\n",
      "Number of characters in smallest text: 18648\n",
      "Key for largest text: DeglA_LabaF_1053655\n",
      "Number of characters in largest text: 2375090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plaintexts are in another repo - private in our parent folder\n",
    "# let's list all text files in data/docs folder\n",
    "# data_folder = Path(\"../data/docs\")\n",
    "# data_folder = Path(\"../../lnb_lat_sen_rom_releases/lat_sen_rom_2025_01_28\")\n",
    "data_folder = Path(\"../../lnb_lat_sen_rom_releases/lat_sen_rom_2025_02_04\")\n",
    "# assert folder exists\n",
    "assert data_folder.exists(), f\"Folder {data_folder} does not exist\"\n",
    "                   \n",
    "# list all files\n",
    "files = list(data_folder.glob(\"*.txt\"))\n",
    "# print all files\n",
    "# how many files do we have?\n",
    "print(f\"Number of files: {len(files)}\")\n",
    "# let's load the files into a dictionary with filename stem as key and text as value\n",
    "# remember to decode the text as utf-8\n",
    "texts = {}\n",
    "for file in tqdm(files):\n",
    "  with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    texts[file.stem] = f.read()\n",
    "# how many texts do we have?\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "# how many characters do we have in total?\n",
    "total_chars = sum([len(text) for text in texts.values()])\n",
    "print(f\"Total characters: {total_chars}\")\n",
    "# what is the smallest text?\n",
    "min_text = min(texts, key=lambda x: len(texts[x]))\n",
    "print(f\"Key for smallest text: {min_text}\")\n",
    "# how many characters does the smallest text have?\n",
    "min_chars = len(texts[min_text])\n",
    "print(f\"Number of characters in smallest text: {min_chars}\")\n",
    "# what is the largest text?\n",
    "max_text = max(texts, key=lambda x: len(texts[x]))\n",
    "print(f\"Key for largest text: {max_text}\")\n",
    "# how many characters does the largest text have?\n",
    "max_chars = len(texts[max_text])\n",
    "print(f\"Number of characters in largest text: {max_chars}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are identical\n"
     ]
    }
   ],
   "source": [
    "# first let's assert that all our folders have identical file names\n",
    "# we will use the first folder as reference\n",
    "reference_files = Path(folders_to_analyze[0]).iterdir()\n",
    "reference_files = [file.name for file in reference_files]\n",
    "for folder in folders_to_analyze[1:]:\n",
    "    files = Path(folder).iterdir()\n",
    "    files = [file.name for file in files]\n",
    "    assert reference_files == files, f\"Files in {folders_to_analyze[0]} and {folder} are not identical\"\n",
    "\n",
    "print(\"All files are identical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function that given a file will extract all response lines\n",
    "# response lines are those that come before empty line\n",
    "# we will return a list of response lines\n",
    "def get_response_lines(file):\n",
    "    response_lines = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.strip() == \"\":\n",
    "                break\n",
    "            response_lines.append(line.strip())\n",
    "    return response_lines\n",
    "\n",
    "# test on first file in reference folder\n",
    "# response_lines = get_response_lines(Path(folders_to_analyze[0]) / reference_files[0])\n",
    "# print(f\"Response lines: {response_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../data/responses/consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt'),\n",
       " WindowsPath('../data/responses/consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2'),\n",
       " WindowsPath('../data/responses/2025_02_26_google_gemini-flash-1.5_land_prompt_1'),\n",
       " WindowsPath('../data/responses/2025_02_26_google_gemini-flash-1.5_land_prompt_2'),\n",
       " WindowsPath('../data/responses/2025_02_27_google_gemini-2.0-flash-001_land_prompt'),\n",
       " WindowsPath('../data/responses/2025_02_27_google_gemini-2.0-flash-001_land_prompt_2')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a function that given a file and texts dictionary will return a dataframe with two columns\n",
    "# first column will be terms sorted from get_response_lines (could be duplicates)\n",
    "# second column will count of occurences of term in text from matching key in texts dictionary\n",
    "# key will be file name stem\n",
    "def get_response_df(file, texts):\n",
    "    response_lines = get_response_lines(file)\n",
    "    data = []\n",
    "    plaintext = texts.get(file.stem, \"\")\n",
    "    # our term column name will be parent folder name of file\n",
    "    term_column = file.parent.name\n",
    "    # term_column = file.stem\n",
    "    if plaintext == \"\":\n",
    "        print(f\"Plaintext not found for {file.stem}\")\n",
    "    for line in sorted(response_lines):\n",
    "        data.append({term_column: line, \"count\": plaintext.count(line)})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# test on first file in reference folder\n",
    "# df = get_response_df(Path(folders_to_analyze[0]) / reference_files[0], texts)\n",
    "# print(f\"Response dataframe:\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AustA_KaspG_948026.txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a function tht given a file and list of subfolders will return a combined dataframe\n",
    "# columns will obtained by horizontally concatenating dataframes obtained by get_response_df\n",
    "# index will be numerical\n",
    "def get_combined_df(file, texts, subfolders):\n",
    "    dfs = []\n",
    "    for subfolder in subfolders:\n",
    "        df = get_response_df(subfolder / file, texts)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "# let's test it on first file in reference folder\n",
    "# df = get_combined_df(reference_files[0], texts, folders_to_analyze)\n",
    "# print(f\"Combined dataframe:\")\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a function that will create a CSV file for each file in reference folder\n",
    "# we will use get_combined_df to get the dataframe\n",
    "# we will supply target folder where we want to save the CSV files\n",
    "def create_csv_files(reference_files, texts, subfolders, target_folder, save_excel=True):\n",
    "    # create target folder if it does not exist\n",
    "    target_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for file in reference_files:\n",
    "        df = get_combined_df(file, texts, subfolders)\n",
    "        df.to_csv(target_folder / f\"{Path(file).stem}.csv\", index=False)\n",
    "        if save_excel:\n",
    "            df.to_excel(target_folder / f\"{Path(file).stem}.xlsx\", index=False)\n",
    "\n",
    "# let's test it on reference files\n",
    "# target folder will be data folder with name analysis and datetime stamp\n",
    "# target_folder = Path(\"../data\") / \"analysis\" / now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "# create_csv_files(reference_files, texts, folders_to_analyze, target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\lnb_lat_sen_rom_releases\\lat_sen_rom_2025_02_04\\analysis\\2025_02_27_21_05_04\n"
     ]
    }
   ],
   "source": [
    "print(target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maritime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maritime folders:\n",
      "..\\data\\responses\\2025_02_27_google_gemini-flash-1.5_maritime_prompt\n"
     ]
    }
   ],
   "source": [
    "# let's get a list of folders that contain words maritime_prompt\n",
    "data_folder = Path(\"../data/responses\")\n",
    "maritime_folders = [f for f in data_folder.iterdir() if f.is_dir() and \"maritime_prompt\" in f.name]\n",
    "print(f\"Maritime folders:\")\n",
    "for folder in maritime_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maritime files:\n",
      "Number of files: 458\n",
      "AizsV_MilaU_1049452.txt\n",
      "AkurJ_DegoS_771400.txt\n",
      "AkurJ_PeteD_886346.txt\n",
      "AkurJ_UgunZ_1049441.txt\n",
      "Andra_Elita_1053573.txt\n"
     ]
    }
   ],
   "source": [
    "# let's get list of files in first maritime folder\n",
    "maritime_files = Path(maritime_folders[0]).iterdir()\n",
    "maritime_files = [file.name for file in maritime_files]\n",
    "print(f\"Maritime files:\")\n",
    "# how many files do we have?\n",
    "print(f\"Number of files: {len(maritime_files)}\")\n",
    "# first 5 files\n",
    "for file in maritime_files[:5]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create_csv_files for maritime files\n",
    "# target folder will be data folder with name analysis_maritime and datetime stamp\n",
    "target_folder = Path(\"../data\") / \"analysis_maritime\" / now.strftime(\"%Y_%m_%d_%H_%M_%S\")   \n",
    "create_csv_files(maritime_files, texts, maritime_folders, target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air transport Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air folders:\n",
      "..\\data\\responses\\2025_02_28_google_gemini-flash-1.5_air_prompt\n"
     ]
    }
   ],
   "source": [
    "# let's get subfolder that contains words air_prompt\n",
    "data_folder = Path(\"../data/responses\")\n",
    "air_folders = [f for f in data_folder.iterdir() if f.is_dir() and \"air_prompt\" in f.name]\n",
    "print(f\"Air folders:\")\n",
    "for folder in air_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get a list of files in first air folder\n",
    "air_files = Path(air_folders[0]).iterdir()\n",
    "air_files = [file.name for file in air_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create csv files for air folders\n",
    "# target folder will be data folder with name analysis_air and datetime stamp\n",
    "target_folder = Path(\"../data\") / \"analysis_air\" / now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "create_csv_files(air_files, texts, air_folders, target_folder, save_excel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining responses into unique response document frequency counts\n",
    "\n",
    "Next we will combine all responses from specific prompt into a single dataframe. We will use a parquet file with lemmatized results to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (37605476, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deprel</th>\n",
       "      <th>form</th>\n",
       "      <th>index</th>\n",
       "      <th>lemma</th>\n",
       "      <th>parent</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>ufeats</th>\n",
       "      <th>upos</th>\n",
       "      <th>sent_ndx</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>dom_id</th>\n",
       "      <th>file_stem</th>\n",
       "      <th>file_stem_short</th>\n",
       "      <th>firstEdition</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>783722</th>\n",
       "      <td>None</td>\n",
       "      <td>dažas</td>\n",
       "      <td>17</td>\n",
       "      <td>daža</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pi0fpa_</td>\n",
       "      <td>pi0fpan</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>978</td>\n",
       "      <td>AustA</td>\n",
       "      <td>GaraJ</td>\n",
       "      <td>1025406</td>\n",
       "      <td>AustA_GaraJ_1025406</td>\n",
       "      <td>AustA_GaraJ</td>\n",
       "      <td>1926</td>\n",
       "      <td>daža</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20251178</th>\n",
       "      <td>root</td>\n",
       "      <td>iebrēcās</td>\n",
       "      <td>16</td>\n",
       "      <td>iebrēkties</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vm_is__30__</td>\n",
       "      <td>vmyis_130an</td>\n",
       "      <td>Evident=Fh|Mood=Ind|Person=3|Polarity=Pos|Refl...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>865</td>\n",
       "      <td>LeitA</td>\n",
       "      <td>TrimA</td>\n",
       "      <td>1296709</td>\n",
       "      <td>LeitA_TrimA_1296709</td>\n",
       "      <td>LeitA_TrimA</td>\n",
       "      <td>1933</td>\n",
       "      <td>iebrēkties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12905910</th>\n",
       "      <td>root</td>\n",
       "      <td>papīrīts</td>\n",
       "      <td>5</td>\n",
       "      <td>papīrīt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vm_pdmsn_sn</td>\n",
       "      <td>vmnpdmsnpsnpn</td>\n",
       "      <td>Aspect=Perf|Case=Nom|Definite=Ind|Degree=Pos|G...</td>\n",
       "      <td>VERB</td>\n",
       "      <td>909</td>\n",
       "      <td>JaunJ</td>\n",
       "      <td>NaveD</td>\n",
       "      <td>1053660</td>\n",
       "      <td>JaunJ_NaveD_1053660</td>\n",
       "      <td>JaunJ_NaveD</td>\n",
       "      <td>1924</td>\n",
       "      <td>papīrīt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6965353</th>\n",
       "      <td>None</td>\n",
       "      <td>nebūtībā</td>\n",
       "      <td>16</td>\n",
       "      <td>nebūtība</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ncfsl_</td>\n",
       "      <td>ncfsl4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>484</td>\n",
       "      <td>ErssA</td>\n",
       "      <td>MileV</td>\n",
       "      <td>1047111</td>\n",
       "      <td>ErssA_MileV_1047111</td>\n",
       "      <td>ErssA_MileV</td>\n",
       "      <td>1930</td>\n",
       "      <td>nebūtība</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23745117</th>\n",
       "      <td>None</td>\n",
       "      <td>klajā</td>\n",
       "      <td>10</td>\n",
       "      <td>klajā</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r_p</td>\n",
       "      <td>r0_</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>133</td>\n",
       "      <td>PaulM</td>\n",
       "      <td>UzDzi</td>\n",
       "      <td>1049498</td>\n",
       "      <td>PaulM_UzDzi_1049498</td>\n",
       "      <td>PaulM_UzDzi</td>\n",
       "      <td>1938</td>\n",
       "      <td>klajā</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         deprel      form  index       lemma  parent          pos  \\\n",
       "783722     None     dažas     17        daža     NaN      pi0fpa_   \n",
       "20251178   root  iebrēcās     16  iebrēkties     0.0  vm_is__30__   \n",
       "12905910   root  papīrīts      5     papīrīt     0.0  vm_pdmsn_sn   \n",
       "6965353    None  nebūtībā     16    nebūtība     NaN       ncfsl_   \n",
       "23745117   None     klajā     10       klajā     NaN          r_p   \n",
       "\n",
       "                    tag                                             ufeats  \\\n",
       "783722          pi0fpan                                               None   \n",
       "20251178    vmyis_130an  Evident=Fh|Mood=Ind|Person=3|Polarity=Pos|Refl...   \n",
       "12905910  vmnpdmsnpsnpn  Aspect=Perf|Case=Nom|Definite=Ind|Degree=Pos|G...   \n",
       "6965353          ncfsl4                                               None   \n",
       "23745117            r0_                                               None   \n",
       "\n",
       "          upos  sent_ndx author  title   dom_id            file_stem  \\\n",
       "783722    None       978  AustA  GaraJ  1025406  AustA_GaraJ_1025406   \n",
       "20251178  VERB       865  LeitA  TrimA  1296709  LeitA_TrimA_1296709   \n",
       "12905910  VERB       909  JaunJ  NaveD  1053660  JaunJ_NaveD_1053660   \n",
       "6965353   None       484  ErssA  MileV  1047111  ErssA_MileV_1047111   \n",
       "23745117  None       133  PaulM  UzDzi  1049498  PaulM_UzDzi_1049498   \n",
       "\n",
       "         file_stem_short  firstEdition        term  \n",
       "783722       AustA_GaraJ          1926        daža  \n",
       "20251178     LeitA_TrimA          1933  iebrēkties  \n",
       "12905910     JaunJ_NaveD          1924     papīrīt  \n",
       "6965353      ErssA_MileV          1930    nebūtība  \n",
       "23745117     PaulM_UzDzi          1938       klajā  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load parquet from outside our repository\n",
    "src = Path(\"../../not_repo/latsenrom_2025_02_05.parquet\")\n",
    "# assert file exists\n",
    "assert src.exists(), f\"File {src} does not exist\"\n",
    "df = pd.read_parquet(src)\n",
    "# shape\n",
    "print(f\"Shape: {df.shape}\")\n",
    "# sample\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stems in air analysis folder: 458\n",
      "Total stems in parquet file: 485\n",
      "Stems in air analysis folder but not in parquet file: {'RoziP_DivaS_1053490', 'JansJ_Dzimt_1051747', 'ZariK_DzivU_1051770', 'MateJ_PatrU_1293506', 'Anoni_BandK_419229', 'CeplA_Zeme_882015', 'JaunJ_Aija_656227', 'KaijI_IedzG_1296798', 'NiedA_LiduD_907728', 'VecoJ_ViktH_1296782', 'DeglA_Zelte_414397', 'JaunJ_BaltG_413159', 'KaijI_Juga_886317', 'UpitA_JaunA_103001', 'NiedA_Siksp_1544155', 'KaudR_MernL_771080', 'EldgH_ZvaiN_771102', 'DeglA_Patri_66131', 'JatnG_TevPi_1049477', 'ZeibJ_BaroB_1293562', 'KaijI_Dzint_1053686', 'UpitA_ZidaT_869211', 'KaijI_Sfink_886333', 'SpriJ_NaveL_1053548', 'MateJ_SadzV_416277'}\n",
      "Stems in parquet file but not in air analysis folder: {'JansJ_Dzim_1051747', 'JatnG_TevPi_049477', 'KaudR_MernL_413085', 'KabeV_DzelD_1049482', 'UpitA_ZemNa_102999', 'UpitA_ZidaT_869228', 'FimbK_PecPu_1051725', 'UpitA_JaunA_1040993', 'RudzE_CaurE_886344', 'CeplA_Zeme1_882015', 'RoziP_DivaS_957619', 'NiedAn_LiduD_413173', 'JaunJ_BaltG_413934', 'MateJ_SadzV_1040863', 'RutkT_MukuB_1053502', 'JekaK_HeinR_1296800', 'GulbA_Zigf_1049519', 'LeitA_TrimA_1296716', 'PaegL_KursM_1053550', 'SaleE_JautR_882013', 'KaijI_IedzG_420132', 'SpriJ_NaveL_33708055', 'JansJ_MezvL_1051791', 'JekaK_Nikol_1053559', 'JaunJ_Aija_413750', 'JekaK_HeinR_1296812', 'DambV_GaitC_1049518', 'MateJ_PatrU_1042665', 'JekaK_HeinR_1296801', 'ZamaL_DireK_957725', 'ZeibJ_BaroB_1040862', 'KaijI_Dzint_1025396', 'EldgH_ZvaiN_419386', 'JansJ_Dzim_1051748', 'JekaK_HeinR_1296810', 'JekaK_HeinR_1296811', 'LaviV_PulaE_1053689', 'DambV_GaitC_1049439', 'ZariK_DzivU_414927', 'JekaK_KrisU_1296793', 'DeglA_LabaF_1053474', 'JansJ_Ligav_1053661', 'NiedAn_Siksp_771071', 'KaijI_Sfink_886320', 'DeglA_Zelte_771027', 'VecoJ_ViktI_1296782', 'KaijI_Juga_886318', 'GulbA_Zigf_1046831', 'LapiK_NemiP_1293516', 'StraK_Kars_1051768', 'GulbA_Zigfr_1049519', 'DeglA_Patri_103063'}\n"
     ]
    }
   ],
   "source": [
    "# let's compare set of file stems in our anaylsis folder with set of stems in our parquet file\n",
    "# we will use set comprehension\n",
    "# we will use set comprehension\n",
    "\n",
    "# list of files in our target folder\n",
    "air_analysis_files = list(target_folder.iterdir())\n",
    "# set of stems in our target folder\n",
    "air_analysis_stems = {file.stem for file in air_analysis_files}\n",
    "# set of stems in our parquet file\n",
    "parquet_stems = set(df[\"file_stem\"].unique())\n",
    "# how many total of each?\n",
    "print(f\"Total stems in air analysis folder: {len(air_analysis_stems)}\")\n",
    "print(f\"Total stems in parquet file: {len(parquet_stems)}\")\n",
    "# let's see the difference\n",
    "print(f\"Stems in air analysis folder but not in parquet file: {air_analysis_stems - parquet_stems}\")\n",
    "print(f\"Stems in parquet file but not in air analysis folder: {parquet_stems - air_analysis_stems}\")\n",
    "\n",
    "# we can see that there have been some changes in the file names but it should not meaningfully affect our analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/analysis_air/2025_02_28_17_50_54')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.errors import EmptyDataError # we need to import this error\n",
    "# now let's create a function that takes a subfolder with responses and returns a dataframe with unique terms and their combined counts\n",
    "# to do so we will go through all csv files in subfolder\n",
    "# we will extract rows with unique terms and their counts\n",
    "# and then we will merge them into a single dataframe by adding counts\n",
    "# we will return this dataframe\n",
    "def get_combined_response_df(subfolder, verbose=False):\n",
    "    dfs = []\n",
    "    for file in tqdm(subfolder.iterdir()):\n",
    "        if file.suffix == \".csv\":\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "            except EmptyDataError:\n",
    "                print(f\"Empty data error for {file}\")\n",
    "                continue\n",
    "            # drop duplicates\n",
    "            df = df.drop_duplicates()\n",
    "            # name first column term\n",
    "            df.columns = [\"term\", \"term_freq\"]\n",
    "            # we also want to add doc_freq column with value 1\n",
    "            df[\"doc_freq\"] = 1\n",
    "            # df = df.groupby(\"term\").sum().reset_index()\n",
    "            dfs.append(df)\n",
    "    if verbose:\n",
    "        # print shape of first 5 dataframes\n",
    "        for df in dfs[:5]:\n",
    "            print(df.shape)\n",
    "    df = pd.concat(dfs).groupby(\"term\").sum().reset_index()\n",
    "    # sort alphabetically\n",
    "    # df = df.sort_values(\"term_freq\", ascending=False)\n",
    "    df = df.sort_values(\"doc_freq\", ascending=False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "458it [00:00, 948.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1744, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>vilciens</td>\n",
       "      <td>566</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>laivas</td>\n",
       "      <td>925</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>auto</td>\n",
       "      <td>1875</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>kamanas</td>\n",
       "      <td>149</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>rati</td>\n",
       "      <td>2222</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>kuģis</td>\n",
       "      <td>480</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>laivu</td>\n",
       "      <td>502</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>lidmašīna</td>\n",
       "      <td>546</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>kuģi</td>\n",
       "      <td>1513</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>tvaikonis</td>\n",
       "      <td>158</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term  term_freq  doc_freq\n",
       "1504   vilciens        566       102\n",
       "771      laivas        925        95\n",
       "160        auto       1875        83\n",
       "582     kamanas        149        71\n",
       "1176       rati       2222        63\n",
       "727       kuģis        480        60\n",
       "786       laivu        502        56\n",
       "825   lidmašīna        546        52\n",
       "722        kuģi       1513        52\n",
       "1386  tvaikonis        158        50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test it on first air folder\n",
    "air_df = get_combined_response_df(target_folder)\n",
    "# shape\n",
    "print(f\"Shape: {air_df.shape}\")\n",
    "#  head 10\n",
    "air_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save this dataframe to a csv file in parent of target folder\n",
    "# the file name will be air_combined with datetime stamp and then .csv\n",
    "# we will save it without index\n",
    "\n",
    "# target file\n",
    "target_file = Path(target_folder).parent / f\"air_combined_{now.strftime('%Y_%m_%d_%H_%M_%S')}.csv\"\n",
    "# save\n",
    "air_df.to_csv(target_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting tf and df for maritime responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maritime folders:\n",
      "..\\data\\analysis_maritime\\2025_02_27_21_05_04\n"
     ]
    }
   ],
   "source": [
    "# let's do the same for maritime folders\n",
    "maritime_target_parent = Path(\"../data\") / \"analysis_maritime\" \n",
    "# get subfolders\n",
    "maritime_folders = [f for f in maritime_target_parent.iterdir() if f.is_dir()]\n",
    "# print\n",
    "print(f\"Maritime folders:\")\n",
    "for folder in maritime_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/analysis_maritime/2025_02_27_21_05_04')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "maritime_target_folder = Path(maritime_folders[0])\n",
    "maritime_target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "604it [00:00, 1956.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty data error for ..\\data\\analysis_maritime\\2025_02_27_21_05_04\\RozeL_StipK_964055.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "916it [00:01, 498.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 3)\n",
      "(5, 3)\n",
      "(7, 3)\n",
      "(14, 3)\n",
      "(2, 3)\n",
      "Shape: (1240, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>laivas</td>\n",
       "      <td>1146</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>kuģis</td>\n",
       "      <td>770</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>kuģi</td>\n",
       "      <td>2492</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>rati</td>\n",
       "      <td>2821</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>laivu</td>\n",
       "      <td>590</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>laivā</td>\n",
       "      <td>521</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>tvaikonis</td>\n",
       "      <td>196</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>vilciens</td>\n",
       "      <td>362</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>kamanas</td>\n",
       "      <td>141</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>laiva</td>\n",
       "      <td>944</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term  term_freq  doc_freq\n",
       "560      laivas       1146       183\n",
       "492       kuģis        770       138\n",
       "485        kuģi       2492       127\n",
       "843        rati       2821        86\n",
       "580       laivu        590        78\n",
       "581       laivā        521        65\n",
       "1003  tvaikonis        196        64\n",
       "1118   vilciens        362        63\n",
       "397     kamanas        141        60\n",
       "558       laiva        944        50"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maritime_df = get_combined_response_df(maritime_target_folder, verbose=True)\n",
    "# shape\n",
    "print(f\"Shape: {maritime_df.shape}\")\n",
    "#  head 10\n",
    "maritime_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save results to maritime_target_parent\n",
    "# target file will have name maritime_term_tf_doc_tf with datetime stamp and then .csv\n",
    "# we will save it without index\n",
    "target_file = maritime_target_parent / f\"maritime_term_tf_doc_tf_{now.strftime('%Y_%m_%d_%H_%M_%S')}.csv\"\n",
    "# save\n",
    "maritime_df.to_csv(target_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
