{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Responses of LLM to prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2025-03-01 12:43:58.715529\n",
      "Python version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "Requests version: 2.32.3\n",
      "Pandas version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "# first we need to import the basic libraries\n",
    "# date\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(f\"Date: {now}\")\n",
    "# python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "from pathlib import Path\n",
    "import json\n",
    "# import time for delay\n",
    "import time\n",
    "import requests\n",
    "# print version\n",
    "print(f\"Requests version: {requests.__version__}\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder: ..\\data\\responses\n",
      "Data folder exists: True\n",
      "Data folder is dir: True\n",
      "Subfolders:\n",
      "..\\data\\responses\\2025_01_28_gemini_2_experimental\n",
      "..\\data\\responses\\2025_01_29_google_gemini-flash-1.5-8b_no_terms\n",
      "..\\data\\responses\\2025_01_29_google_gemini-flash-1.5-8b_with_terms\n",
      "..\\data\\responses\\2025_02_04_google_gemini-flash-1.5-8b_with_terms\n",
      "..\\data\\responses\\2025_02_26_google_gemini-flash-1.5_land_prompt_1\n",
      "..\\data\\responses\\2025_02_26_google_gemini-flash-1.5_land_prompt_2\n",
      "..\\data\\responses\\2025_02_27_google_gemini-2.0-flash-001_land_prompt\n",
      "..\\data\\responses\\2025_02_27_google_gemini-2.0-flash-001_land_prompt_2\n",
      "..\\data\\responses\\2025_02_27_google_gemini-flash-1.5_maritime_prompt\n",
      "..\\data\\responses\\2025_02_28_google_gemini-flash-1.5_air_prompt\n",
      "..\\data\\responses\\consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt\n",
      "..\\data\\responses\\consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2\n",
      "..\\data\\responses\\temp_responses_2025_02_26\n",
      "..\\data\\responses\\unconsolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt\n",
      "..\\data\\responses\\unconsolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2\n"
     ]
    }
   ],
   "source": [
    "# let's see what folders are in our ../data/responses folder\n",
    "data_folder = Path(\"../data/responses\")\n",
    "print(f\"Data folder: {data_folder}\")\n",
    "print(f\"Data folder exists: {data_folder.exists()}\")\n",
    "print(f\"Data folder is dir: {data_folder.is_dir()}\")\n",
    "# let's see what subfolders are in our data folder\n",
    "subfolders = [f for f in data_folder.iterdir() if f.is_dir()]\n",
    "print(f\"Subfolders:\")\n",
    "for subfolder in subfolders:\n",
    "    print(subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate openai responses\n",
    "\n",
    "OpenAI prompts required us to break down files into smaller chunks, now we need to consolidate them back into a single file.\n",
    "\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI folders:\n",
      "..\\data\\responses\\2025_02_26_openai_gpt-4o-2024-11-20_land_prompt\n",
      "..\\data\\responses\\2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2\n"
     ]
    }
   ],
   "source": [
    "# subfolders that contain openai in their name\n",
    "openai_folders = [f for f in data_folder.iterdir() if f.is_dir() and \"openai\" in f.name]\n",
    "print(f\"OpenAI folders:\")\n",
    "for openai_folder in openai_folders:\n",
    "    print(openai_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI files:\n",
      "AustA_KaspG_948026: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/AustA_KaspG_948026_0.txt')]\n",
      "AustA_Puisk_1047362: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/AustA_Puisk_1047362_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/AustA_Puisk_1047362_1.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/AustA_Puisk_1047362_2.txt')]\n",
      "FimbK_KadNa_1049450: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/FimbK_KadNa_1049450_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/FimbK_KadNa_1049450_1.txt')]\n",
      "FimbK_TiltP_1049479: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/FimbK_TiltP_1049479_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/FimbK_TiltP_1049479_1.txt')]\n",
      "GulbA_Gaidi_1350352: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/GulbA_Gaidi_1350352_0.txt')]\n",
      "GulbA_JaunV_1053680: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/GulbA_JaunV_1053680_0.txt')]\n",
      "LaciV_AtbrZ_1051755: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LaciV_AtbrZ_1051755_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LaciV_AtbrZ_1051755_1.txt')]\n",
      "LaciV_PulaE_1053689: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LaciV_PulaE_1053689_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LaciV_PulaE_1053689_1.txt')]\n",
      "LapiK_DodaU_1046848: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LapiK_DodaU_1046848_0.txt')]\n",
      "LapiK_ManaD_1051717: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LapiK_ManaD_1051717_0.txt')]\n",
      "LesiVi_LiktR_1051787: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LesiVi_LiktR_1051787_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LesiVi_LiktR_1051787_1.txt')]\n",
      "LesiVi_Uzvar_1350343: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LesiVi_Uzvar_1350343_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/LesiVi_Uzvar_1350343_1.txt')]\n",
      "NiedAi_CilvA_1025449: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/NiedAi_CilvA_1025449_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/NiedAi_CilvA_1025449_1.txt')]\n",
      "NiedAi_Salna_1049440: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/NiedAi_Salna_1049440_0.txt')]\n",
      "PaulM_SirdP_1049495: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/PaulM_SirdP_1049495_0.txt')]\n",
      "PaulM_VienV_1049499: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/PaulM_VienV_1049499_0.txt')]\n",
      "RoziP_DivaS_1053490: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/RoziP_DivaS_1053490_0.txt')]\n",
      "RoziP_UgunC_1046826: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/RoziP_UgunC_1046826_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/RoziP_UgunC_1046826_1.txt')]\n",
      "SartJ_Pagar_1047279: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/SartJ_Pagar_1047279_0.txt'), WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/SartJ_Pagar_1047279_1.txt')]\n",
      "SartJ_Zieda_1051763: [WindowsPath('../data/responses/2025_02_26_openai_gpt-4o-2024-11-20_land_prompt/SartJ_Zieda_1051763_0.txt')]\n"
     ]
    }
   ],
   "source": [
    "# we want to create a function that given a subfolder will return a dictionary \n",
    "# keys will be first three parts of file name when split by _\n",
    "# values will be actual file names\n",
    "def get_files(subfolder):\n",
    "    files = {}\n",
    "    for file in subfolder.iterdir():\n",
    "        if file.is_file():\n",
    "            parts = file.name.split(\"_\")\n",
    "            key = \"_\".join(parts[:3])\n",
    "            if key in files:\n",
    "                files[key].append(file)\n",
    "            else:\n",
    "                files[key] = [file]\n",
    "    return files\n",
    "\n",
    "# let's run this function on one of the openai folders\n",
    "openai_files = get_files(openai_folders[0])\n",
    "print(f\"OpenAI files:\")\n",
    "for key, value in openai_files.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025_02_26_openai_gpt-4o-2024-11-20_land_prompt: 20\n",
      "2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2: 20\n"
     ]
    }
   ],
   "source": [
    "# let's run get_files on all openai folders\n",
    "# the key will be folder name and values will be dictionaries returned by get_files\n",
    "openai_files = {}\n",
    "for openai_folder in openai_folders:\n",
    "    openai_files[openai_folder.name] = get_files(openai_folder)\n",
    "\n",
    "# how many files are in each folder\n",
    "for key, value in openai_files.items():\n",
    "    print(f\"{key}: {len(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a function that given a file name and file list and new_subfolder will write consolidated file to new_subfolder\n",
    "# logic is as follows:\n",
    "# we want to read all content of files in file list up to empty line\n",
    "# we want to write all this content to new file in new_subfolder\n",
    "# then we want to separately read all lines starting with line that starts with \"System prompt:\"\n",
    "# we want to write this content only once to new file in new_subfolder\n",
    "# we want use utf-8 encoding\n",
    "def consolidate_files(file_name, file_list, new_subfolder):\n",
    "    # create new subfolder if it does not exist\n",
    "    new_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "    with open(new_subfolder / f\"{file_name}.txt\", \"w\", encoding=\"utf-8\") as new_file:\n",
    "        system_prompts = []\n",
    "        for file in file_list:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as old_file:\n",
    "                text = old_file.read()\n",
    "                # let's split on \"System prompt:\"\n",
    "                parts = text.split(\"System prompt:\")\n",
    "                # let's write first part\n",
    "                new_file.write(parts[0].strip()+\"\\n\")\n",
    "                # append second part to system_prompts\n",
    "                system_prompts.append(parts[1])\n",
    "        # let's write system prompts only once\n",
    "        # first check if system prompts are identical\n",
    "        if len(set(system_prompts)) == 1:\n",
    "            new_file.write(\"\\nSystem prompt:\" + system_prompts[0])\n",
    "        else:\n",
    "            for system_prompt in system_prompts:\n",
    "                new_file.write(\"System prompt:\\n\" + system_prompt)\n",
    "\n",
    "# test it on second key of openai_files\n",
    "# we will create a new subfolder in the data respones folder\n",
    "# new_subfolder = data_folder / \"consolidated\"\n",
    "# consolidate_files(list(openai_files.keys())[1], openai_files[list(openai_files.keys())[1]], new_subfolder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a function that will consolidate all files in all openai folders\n",
    "# we will use consolidate_files function\n",
    "# new subfolder will be in data folder\n",
    "# it will be called consolidated_ + key of openai_files\n",
    "def consolidate_all_files(openai_files, data_folder):\n",
    "    for key, value in openai_files.items():\n",
    "        new_subfolder = data_folder / (\"consolidated_\" + key)\n",
    "        # value is a dictionary that contains keys that are first three parts of file name and values that are lists of files\n",
    "        for key2, value2 in value.items():\n",
    "            consolidate_files(key2, value2, new_subfolder)\n",
    "\n",
    "# let's run this function\n",
    "consolidate_all_files(openai_files, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the subfolders for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders to analyze:\n",
      "..\\data\\responses\\consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt\n",
      "..\\data\\responses\\consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2\n",
      "..\\data\\responses\\2025_02_26_google_gemini-flash-1.5_land_prompt_1\n",
      "..\\data\\responses\\2025_02_26_google_gemini-flash-1.5_land_prompt_2\n",
      "..\\data\\responses\\2025_02_27_google_gemini-2.0-flash-001_land_prompt\n",
      "..\\data\\responses\\2025_02_27_google_gemini-2.0-flash-001_land_prompt_2\n"
     ]
    }
   ],
   "source": [
    "# now we want to get all folders that we want to analyze\n",
    "# they are in data_folder \n",
    "# we want those that start with consolidated_2025_02_26 or consolidated_2025_02_27\n",
    "# we also want those that start with 2025_02_26 or 2025_02_27 and also contain words land_prompt\n",
    "# these will be the folders that we want to analyze\n",
    "folders_to_analyze = [f for f in data_folder.iterdir() if f.is_dir() and (f.name.startswith(\"consolidated_2025_02_26\") or f.name.startswith(\"consolidated_2025_02_27\"))]\n",
    "folders_to_analyze += [f for f in data_folder.iterdir() if f.is_dir() and (f.name.startswith(\"2025_02_26\") or f.name.startswith(\"2025_02_27\")) and \"land_prompt\" in f.name]\n",
    "print(f\"Folders to analyze:\")\n",
    "for folder in folders_to_analyze:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Plaintext into  memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [00:07<00:00, 61.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 458\n",
      "Total characters: 191069647\n",
      "Key for smallest text: VentA_DepuT_1293527\n",
      "Number of characters in smallest text: 18648\n",
      "Key for largest text: DeglA_LabaF_1053655\n",
      "Number of characters in largest text: 2375090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plaintexts are in another repo - private in our parent folder\n",
    "# let's list all text files in data/docs folder\n",
    "# data_folder = Path(\"../data/docs\")\n",
    "# data_folder = Path(\"../../lnb_lat_sen_rom_releases/lat_sen_rom_2025_01_28\")\n",
    "data_folder = Path(\"../../lnb_lat_sen_rom_releases/lat_sen_rom_2025_02_04\")\n",
    "# assert folder exists\n",
    "assert data_folder.exists(), f\"Folder {data_folder} does not exist\"\n",
    "                   \n",
    "# list all files\n",
    "files = list(data_folder.glob(\"*.txt\"))\n",
    "# print all files\n",
    "# how many files do we have?\n",
    "print(f\"Number of files: {len(files)}\")\n",
    "# let's load the files into a dictionary with filename stem as key and text as value\n",
    "# remember to decode the text as utf-8\n",
    "texts = {}\n",
    "for file in tqdm(files):\n",
    "  with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    texts[file.stem] = f.read()\n",
    "# how many texts do we have?\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "# how many characters do we have in total?\n",
    "total_chars = sum([len(text) for text in texts.values()])\n",
    "print(f\"Total characters: {total_chars}\")\n",
    "# what is the smallest text?\n",
    "min_text = min(texts, key=lambda x: len(texts[x]))\n",
    "print(f\"Key for smallest text: {min_text}\")\n",
    "# how many characters does the smallest text have?\n",
    "min_chars = len(texts[min_text])\n",
    "print(f\"Number of characters in smallest text: {min_chars}\")\n",
    "# what is the largest text?\n",
    "max_text = max(texts, key=lambda x: len(texts[x]))\n",
    "print(f\"Key for largest text: {max_text}\")\n",
    "# how many characters does the largest text have?\n",
    "max_chars = len(texts[max_text])\n",
    "print(f\"Number of characters in largest text: {max_chars}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are identical\n"
     ]
    }
   ],
   "source": [
    "# first let's assert that all our folders have identical file names\n",
    "# we will use the first folder as reference\n",
    "reference_files = Path(folders_to_analyze[0]).iterdir()\n",
    "reference_files = [file.name for file in reference_files]\n",
    "for folder in folders_to_analyze[1:]:\n",
    "    files = Path(folder).iterdir()\n",
    "    files = [file.name for file in files]\n",
    "    assert reference_files == files, f\"Files in {folders_to_analyze[0]} and {folder} are not identical\"\n",
    "\n",
    "print(\"All files are identical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function that given a file will extract all response lines\n",
    "# response lines are those that come before empty line\n",
    "# we will return a list of response lines\n",
    "def get_response_lines(file):\n",
    "    response_lines = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.strip() == \"\":\n",
    "                break\n",
    "            response_lines.append(line.strip())\n",
    "    return response_lines\n",
    "\n",
    "# test on first file in reference folder\n",
    "# response_lines = get_response_lines(Path(folders_to_analyze[0]) / reference_files[0])\n",
    "# print(f\"Response lines: {response_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../data/responses/consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt'),\n",
       " WindowsPath('../data/responses/consolidated_2025_02_26_openai_gpt-4o-2024-11-20_land_prompt_2'),\n",
       " WindowsPath('../data/responses/2025_02_26_google_gemini-flash-1.5_land_prompt_1'),\n",
       " WindowsPath('../data/responses/2025_02_26_google_gemini-flash-1.5_land_prompt_2'),\n",
       " WindowsPath('../data/responses/2025_02_27_google_gemini-2.0-flash-001_land_prompt'),\n",
       " WindowsPath('../data/responses/2025_02_27_google_gemini-2.0-flash-001_land_prompt_2')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a function that given a file and texts dictionary will return a dataframe with two columns\n",
    "# first column will be terms sorted from get_response_lines (could be duplicates)\n",
    "# second column will count of occurences of term in text from matching key in texts dictionary\n",
    "# key will be file name stem\n",
    "def get_response_df(file, texts):\n",
    "    response_lines = get_response_lines(file)\n",
    "    data = []\n",
    "    plaintext = texts.get(file.stem, \"\")\n",
    "    # our term column name will be parent folder name of file\n",
    "    term_column = file.parent.name\n",
    "    # term_column = file.stem\n",
    "    if plaintext == \"\":\n",
    "        print(f\"Plaintext not found for {file.stem}\")\n",
    "    for line in sorted(response_lines):\n",
    "        data.append({term_column: line, \"count\": plaintext.count(line)})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# test on first file in reference folder\n",
    "# df = get_response_df(Path(folders_to_analyze[0]) / reference_files[0], texts)\n",
    "# print(f\"Response dataframe:\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AustA_KaspG_948026.txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a function tht given a file and list of subfolders will return a combined dataframe\n",
    "# columns will obtained by horizontally concatenating dataframes obtained by get_response_df\n",
    "# index will be numerical\n",
    "def get_combined_df(file, texts, subfolders):\n",
    "    dfs = []\n",
    "    for subfolder in subfolders:\n",
    "        df = get_response_df(subfolder / file, texts)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "# let's test it on first file in reference folder\n",
    "# df = get_combined_df(reference_files[0], texts, folders_to_analyze)\n",
    "# print(f\"Combined dataframe:\")\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a function that will create a CSV file for each file in reference folder\n",
    "# we will use get_combined_df to get the dataframe\n",
    "# we will supply target folder where we want to save the CSV files\n",
    "def create_csv_files(reference_files, texts, subfolders, target_folder, save_excel=True):\n",
    "    # create target folder if it does not exist\n",
    "    target_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for file in reference_files:\n",
    "        df = get_combined_df(file, texts, subfolders)\n",
    "        df.to_csv(target_folder / f\"{Path(file).stem}.csv\", index=False)\n",
    "        if save_excel:\n",
    "            df.to_excel(target_folder / f\"{Path(file).stem}.xlsx\", index=False)\n",
    "\n",
    "# let's test it on reference files\n",
    "# target folder will be data folder with name analysis and datetime stamp\n",
    "# target_folder = Path(\"../data\") / \"analysis\" / now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "# create_csv_files(reference_files, texts, folders_to_analyze, target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\lnb_lat_sen_rom_releases\\lat_sen_rom_2025_02_04\\analysis\\2025_02_27_21_05_04\n"
     ]
    }
   ],
   "source": [
    "print(target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maritime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maritime folders:\n",
      "..\\data\\responses\\2025_02_27_google_gemini-flash-1.5_maritime_prompt\n"
     ]
    }
   ],
   "source": [
    "# let's get a list of folders that contain words maritime_prompt\n",
    "data_folder = Path(\"../data/responses\")\n",
    "maritime_folders = [f for f in data_folder.iterdir() if f.is_dir() and \"maritime_prompt\" in f.name]\n",
    "print(f\"Maritime folders:\")\n",
    "for folder in maritime_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maritime files:\n",
      "Number of files: 458\n",
      "AizsV_MilaU_1049452.txt\n",
      "AkurJ_DegoS_771400.txt\n",
      "AkurJ_PeteD_886346.txt\n",
      "AkurJ_UgunZ_1049441.txt\n",
      "Andra_Elita_1053573.txt\n"
     ]
    }
   ],
   "source": [
    "# let's get list of files in first maritime folder\n",
    "maritime_files = Path(maritime_folders[0]).iterdir()\n",
    "maritime_files = [file.name for file in maritime_files]\n",
    "print(f\"Maritime files:\")\n",
    "# how many files do we have?\n",
    "print(f\"Number of files: {len(maritime_files)}\")\n",
    "# first 5 files\n",
    "for file in maritime_files[:5]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create_csv_files for maritime files\n",
    "# target folder will be data folder with name analysis_maritime and datetime stamp\n",
    "target_folder = Path(\"../data\") / \"analysis_maritime\" / now.strftime(\"%Y_%m_%d_%H_%M_%S\")   \n",
    "create_csv_files(maritime_files, texts, maritime_folders, target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air transport Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air folders:\n",
      "..\\data\\responses\\2025_02_28_google_gemini-flash-1.5_air_prompt\n"
     ]
    }
   ],
   "source": [
    "# let's get subfolder that contains words air_prompt\n",
    "data_folder = Path(\"../data/responses\")\n",
    "air_folders = [f for f in data_folder.iterdir() if f.is_dir() and \"air_prompt\" in f.name]\n",
    "print(f\"Air folders:\")\n",
    "for folder in air_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get a list of files in first air folder\n",
    "air_files = Path(air_folders[0]).iterdir()\n",
    "air_files = [file.name for file in air_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create csv files for air folders\n",
    "# target folder will be data folder with name analysis_air and datetime stamp\n",
    "target_folder = Path(\"../data\") / \"analysis_air\" / now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "create_csv_files(air_files, texts, air_folders, target_folder, save_excel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading parquet file with lemma and pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before loading: 129.90234375 MB\n",
      "Memory usage after loading: 12099.02734375 MB\n",
      "Shape: (37605476, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deprel</th>\n",
       "      <th>form</th>\n",
       "      <th>index</th>\n",
       "      <th>lemma</th>\n",
       "      <th>parent</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>ufeats</th>\n",
       "      <th>upos</th>\n",
       "      <th>sent_ndx</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>dom_id</th>\n",
       "      <th>file_stem</th>\n",
       "      <th>file_stem_short</th>\n",
       "      <th>firstEdition</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17154210</th>\n",
       "      <td>None</td>\n",
       "      <td>rokā</td>\n",
       "      <td>6</td>\n",
       "      <td>roka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ncfsl_</td>\n",
       "      <td>ncfsl4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>775</td>\n",
       "      <td>LaciJ</td>\n",
       "      <td>MuzaM</td>\n",
       "      <td>963944</td>\n",
       "      <td>LaciJ_MuzaM_963944</td>\n",
       "      <td>LaciJ_MuzaM</td>\n",
       "      <td>1936</td>\n",
       "      <td>roka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35637918</th>\n",
       "      <td>None</td>\n",
       "      <td>rokas</td>\n",
       "      <td>4</td>\n",
       "      <td>roka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ncfpn_</td>\n",
       "      <td>ncfpn4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>927</td>\n",
       "      <td>ZiemV</td>\n",
       "      <td>ZemBe</td>\n",
       "      <td>1049480</td>\n",
       "      <td>ZiemV_ZemBe_1049480</td>\n",
       "      <td>ZiemV_ZemBe</td>\n",
       "      <td>1934</td>\n",
       "      <td>roka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13721680</th>\n",
       "      <td>conj</td>\n",
       "      <td>Kristine</td>\n",
       "      <td>9</td>\n",
       "      <td>Kristine</td>\n",
       "      <td>7.0</td>\n",
       "      <td>npfsn_</td>\n",
       "      <td>npfsn5</td>\n",
       "      <td>Case=Nom|Gender=Fem|Number=Sing</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>387</td>\n",
       "      <td>JekaK</td>\n",
       "      <td>KrisU</td>\n",
       "      <td>1296792</td>\n",
       "      <td>JekaK_KrisU_1296792</td>\n",
       "      <td>JekaK_KrisU</td>\n",
       "      <td>1930</td>\n",
       "      <td>Kristine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37883302</th>\n",
       "      <td>punct</td>\n",
       "      <td>!</td>\n",
       "      <td>11</td>\n",
       "      <td>!</td>\n",
       "      <td>10.0</td>\n",
       "      <td>zs</td>\n",
       "      <td>zs</td>\n",
       "      <td>_</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>140</td>\n",
       "      <td>NiedAi</td>\n",
       "      <td>PiekB</td>\n",
       "      <td>1049492</td>\n",
       "      <td>NiedAi_PiekB_1049492</td>\n",
       "      <td>NiedAi_PiekB</td>\n",
       "      <td>1931</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11644067</th>\n",
       "      <td>None</td>\n",
       "      <td>.</td>\n",
       "      <td>67</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zs</td>\n",
       "      <td>zs</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>79</td>\n",
       "      <td>JansJ</td>\n",
       "      <td>Ligav</td>\n",
       "      <td>1053661</td>\n",
       "      <td>JansJ_Ligav_1053661</td>\n",
       "      <td>JansJ_Ligav</td>\n",
       "      <td>1932</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         deprel      form  index     lemma  parent     pos     tag  \\\n",
       "17154210   None      rokā      6      roka     NaN  ncfsl_  ncfsl4   \n",
       "35637918   None     rokas      4      roka     NaN  ncfpn_  ncfpn4   \n",
       "13721680   conj  Kristine      9  Kristine     7.0  npfsn_  npfsn5   \n",
       "37883302  punct         !     11         !    10.0      zs      zs   \n",
       "11644067   None         .     67         .     NaN      zs      zs   \n",
       "\n",
       "                                   ufeats   upos  sent_ndx  author  title  \\\n",
       "17154210                             None   None       775   LaciJ  MuzaM   \n",
       "35637918                             None   None       927   ZiemV  ZemBe   \n",
       "13721680  Case=Nom|Gender=Fem|Number=Sing  PROPN       387   JekaK  KrisU   \n",
       "37883302                                _  PUNCT       140  NiedAi  PiekB   \n",
       "11644067                             None   None        79   JansJ  Ligav   \n",
       "\n",
       "           dom_id             file_stem file_stem_short  firstEdition  \\\n",
       "17154210   963944    LaciJ_MuzaM_963944     LaciJ_MuzaM          1936   \n",
       "35637918  1049480   ZiemV_ZemBe_1049480     ZiemV_ZemBe          1934   \n",
       "13721680  1296792   JekaK_KrisU_1296792     JekaK_KrisU          1930   \n",
       "37883302  1049492  NiedAi_PiekB_1049492    NiedAi_PiekB          1931   \n",
       "11644067  1053661   JansJ_Ligav_1053661     JansJ_Ligav          1932   \n",
       "\n",
       "              term  \n",
       "17154210      roka  \n",
       "35637918      roka  \n",
       "13721680  Kristine  \n",
       "37883302         !  \n",
       "11644067         .  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load parquet from outside our repository\n",
    "src = Path(\"../../not_repo/latsenrom_2025_02_05.parquet\")\n",
    "# assert file exists\n",
    "assert src.exists(), f\"File {src} does not exist\"\n",
    "# memory usage before loading\n",
    "import os\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(f\"Memory usage before loading: {process.memory_info().rss / 1024**2} MB\")\n",
    "# load parquet\n",
    "df = pd.read_parquet(src)\n",
    "# memory usage after loading\n",
    "print(f\"Memory usage after loading: {process.memory_info().rss / 1024**2} MB\")\n",
    "# shape\n",
    "print(f\"Shape: {df.shape}\")\n",
    "# sample\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique forms: 1249151\n",
      "Unique lemmas: 777635\n"
     ]
    }
   ],
   "source": [
    "# how many unique form and how many lemma do we have?\n",
    "unique_forms = df[\"form\"].nunique()\n",
    "unique_lemmas = df[\"lemma\"].nunique()\n",
    "print(f\"Unique forms: {unique_forms}\")\n",
    "print(f\"Unique lemmas: {unique_lemmas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deprel</th>\n",
       "      <th>form</th>\n",
       "      <th>index</th>\n",
       "      <th>lemma</th>\n",
       "      <th>parent</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>ufeats</th>\n",
       "      <th>upos</th>\n",
       "      <th>sent_ndx</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>dom_id</th>\n",
       "      <th>file_stem</th>\n",
       "      <th>file_stem_short</th>\n",
       "      <th>firstEdition</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>872755</th>\n",
       "      <td>None</td>\n",
       "      <td>kuģitī</td>\n",
       "      <td>2</td>\n",
       "      <td>kuģitī</td>\n",
       "      <td>NaN</td>\n",
       "      <td>np000_</td>\n",
       "      <td>nc___0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>91</td>\n",
       "      <td>AustA</td>\n",
       "      <td>GaraJ</td>\n",
       "      <td>1025406</td>\n",
       "      <td>AustA_GaraJ_1025406</td>\n",
       "      <td>AustA_GaraJ</td>\n",
       "      <td>1926</td>\n",
       "      <td>kuģitī</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       deprel    form  index   lemma  parent     pos     tag ufeats  upos  \\\n",
       "872755   None  kuģitī      2  kuģitī     NaN  np000_  nc___0   None  None   \n",
       "\n",
       "        sent_ndx author  title   dom_id            file_stem file_stem_short  \\\n",
       "872755        91  AustA  GaraJ  1025406  AustA_GaraJ_1025406     AustA_GaraJ   \n",
       "\n",
       "        firstEdition    term  \n",
       "872755          1926  kuģitī  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needle = \"kuģitī\"\n",
    "# print sample of 10 rows where lemma is needle\n",
    "df[df[\"lemma\"] == needle].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deprel</th>\n",
       "      <th>form</th>\n",
       "      <th>index</th>\n",
       "      <th>lemma</th>\n",
       "      <th>parent</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>ufeats</th>\n",
       "      <th>upos</th>\n",
       "      <th>sent_ndx</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>dom_id</th>\n",
       "      <th>file_stem</th>\n",
       "      <th>file_stem_short</th>\n",
       "      <th>firstEdition</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>872755</th>\n",
       "      <td>None</td>\n",
       "      <td>kuģitī</td>\n",
       "      <td>2</td>\n",
       "      <td>kuģitī</td>\n",
       "      <td>NaN</td>\n",
       "      <td>np000_</td>\n",
       "      <td>nc___0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>91</td>\n",
       "      <td>AustA</td>\n",
       "      <td>GaraJ</td>\n",
       "      <td>1025406</td>\n",
       "      <td>AustA_GaraJ_1025406</td>\n",
       "      <td>AustA_GaraJ</td>\n",
       "      <td>1926</td>\n",
       "      <td>kuģitī</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070454</th>\n",
       "      <td>obl</td>\n",
       "      <td>kuģitī</td>\n",
       "      <td>3</td>\n",
       "      <td>kuģitis</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ncmsl_</td>\n",
       "      <td>ncmsl2</td>\n",
       "      <td>Case=Loc|Gender=Masc|Number=Sing</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>997</td>\n",
       "      <td>EgliV</td>\n",
       "      <td>SkolK</td>\n",
       "      <td>771017</td>\n",
       "      <td>EgliV_SkolK_771017</td>\n",
       "      <td>EgliV_SkolK</td>\n",
       "      <td>1921</td>\n",
       "      <td>kuģitis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101521</th>\n",
       "      <td>obl</td>\n",
       "      <td>kuģitī</td>\n",
       "      <td>4</td>\n",
       "      <td>kuģitis</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ncmsl_</td>\n",
       "      <td>ncmsl2</td>\n",
       "      <td>Case=Loc|Gender=Masc|Number=Sing</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>601</td>\n",
       "      <td>EgliV</td>\n",
       "      <td>SkolK</td>\n",
       "      <td>771017</td>\n",
       "      <td>EgliV_SkolK_771017</td>\n",
       "      <td>EgliV_SkolK</td>\n",
       "      <td>1921</td>\n",
       "      <td>kuģitis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10752258</th>\n",
       "      <td>None</td>\n",
       "      <td>kuģitī</td>\n",
       "      <td>20</td>\n",
       "      <td>kuģitis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>npmsl_</td>\n",
       "      <td>ncmsl2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>638</td>\n",
       "      <td>IeviK</td>\n",
       "      <td>SievM</td>\n",
       "      <td>1051668</td>\n",
       "      <td>IeviK_SievM_1051668</td>\n",
       "      <td>IeviK_SievM</td>\n",
       "      <td>1926</td>\n",
       "      <td>kuģitis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16626927</th>\n",
       "      <td>None</td>\n",
       "      <td>kuģitī</td>\n",
       "      <td>8</td>\n",
       "      <td>kuģitis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ncmsl_</td>\n",
       "      <td>ncmsl2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>713</td>\n",
       "      <td>KukuJ</td>\n",
       "      <td>Laimi</td>\n",
       "      <td>1058165</td>\n",
       "      <td>KukuJ_Laimi_1058165</td>\n",
       "      <td>KukuJ_Laimi</td>\n",
       "      <td>1939</td>\n",
       "      <td>kuģitis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         deprel    form  index    lemma  parent     pos     tag  \\\n",
       "872755     None  kuģitī      2   kuģitī     NaN  np000_  nc___0   \n",
       "6070454     obl  kuģitī      3  kuģitis     2.0  ncmsl_  ncmsl2   \n",
       "6101521     obl  kuģitī      4  kuģitis     3.0  ncmsl_  ncmsl2   \n",
       "10752258   None  kuģitī     20  kuģitis     NaN  npmsl_  ncmsl2   \n",
       "16626927   None  kuģitī      8  kuģitis     NaN  ncmsl_  ncmsl2   \n",
       "\n",
       "                                    ufeats  upos  sent_ndx author  title  \\\n",
       "872755                                None  None        91  AustA  GaraJ   \n",
       "6070454   Case=Loc|Gender=Masc|Number=Sing  NOUN       997  EgliV  SkolK   \n",
       "6101521   Case=Loc|Gender=Masc|Number=Sing  NOUN       601  EgliV  SkolK   \n",
       "10752258                              None  None       638  IeviK  SievM   \n",
       "16626927                              None  None       713  KukuJ  Laimi   \n",
       "\n",
       "           dom_id            file_stem file_stem_short  firstEdition     term  \n",
       "872755    1025406  AustA_GaraJ_1025406     AustA_GaraJ          1926   kuģitī  \n",
       "6070454    771017   EgliV_SkolK_771017     EgliV_SkolK          1921  kuģitis  \n",
       "6101521    771017   EgliV_SkolK_771017     EgliV_SkolK          1921  kuģitis  \n",
       "10752258  1051668  IeviK_SievM_1051668     IeviK_SievM          1926  kuģitis  \n",
       "16626927  1058165  KukuJ_Laimi_1058165     KukuJ_Laimi          1939  kuģitis  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how about where form is needle?\n",
    "df[df[\"form\"] == needle].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that some lemmas are not in the form we are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape where lemma is kuģītis: (205, 17)\n",
      "Shape where form is kuģītis: (27, 17)\n"
     ]
    }
   ],
   "source": [
    "# let's check the shape of different needle = \"kuģītis\"\n",
    "needle = \"kuģītis\"\n",
    "print(f\"Shape where lemma is {needle}: {df[df['lemma'] == needle].shape}\")\n",
    "print(f\"Shape where form is {needle}: {df[df['form'] == needle].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining responses into unique response document frequency counts\n",
    "\n",
    "Next we will combine all responses from specific prompt into a single dataframe. We will use a parquet file with lemmatized results to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stems in air analysis folder: 458\n",
      "Total stems in parquet file: 485\n",
      "Stems in air analysis folder but not in parquet file: {'RoziP_DivaS_1053490', 'JansJ_Dzimt_1051747', 'ZariK_DzivU_1051770', 'MateJ_PatrU_1293506', 'Anoni_BandK_419229', 'CeplA_Zeme_882015', 'JaunJ_Aija_656227', 'KaijI_IedzG_1296798', 'NiedA_LiduD_907728', 'VecoJ_ViktH_1296782', 'DeglA_Zelte_414397', 'JaunJ_BaltG_413159', 'KaijI_Juga_886317', 'UpitA_JaunA_103001', 'NiedA_Siksp_1544155', 'KaudR_MernL_771080', 'EldgH_ZvaiN_771102', 'DeglA_Patri_66131', 'JatnG_TevPi_1049477', 'ZeibJ_BaroB_1293562', 'KaijI_Dzint_1053686', 'UpitA_ZidaT_869211', 'KaijI_Sfink_886333', 'SpriJ_NaveL_1053548', 'MateJ_SadzV_416277'}\n",
      "Stems in parquet file but not in air analysis folder: {'JansJ_Dzim_1051747', 'JatnG_TevPi_049477', 'KaudR_MernL_413085', 'KabeV_DzelD_1049482', 'UpitA_ZemNa_102999', 'UpitA_ZidaT_869228', 'FimbK_PecPu_1051725', 'UpitA_JaunA_1040993', 'RudzE_CaurE_886344', 'CeplA_Zeme1_882015', 'RoziP_DivaS_957619', 'NiedAn_LiduD_413173', 'JaunJ_BaltG_413934', 'MateJ_SadzV_1040863', 'RutkT_MukuB_1053502', 'JekaK_HeinR_1296800', 'GulbA_Zigf_1049519', 'LeitA_TrimA_1296716', 'PaegL_KursM_1053550', 'SaleE_JautR_882013', 'KaijI_IedzG_420132', 'SpriJ_NaveL_33708055', 'JansJ_MezvL_1051791', 'JekaK_Nikol_1053559', 'JaunJ_Aija_413750', 'JekaK_HeinR_1296812', 'DambV_GaitC_1049518', 'MateJ_PatrU_1042665', 'JekaK_HeinR_1296801', 'ZamaL_DireK_957725', 'ZeibJ_BaroB_1040862', 'KaijI_Dzint_1025396', 'EldgH_ZvaiN_419386', 'JansJ_Dzim_1051748', 'JekaK_HeinR_1296810', 'JekaK_HeinR_1296811', 'LaviV_PulaE_1053689', 'DambV_GaitC_1049439', 'ZariK_DzivU_414927', 'JekaK_KrisU_1296793', 'DeglA_LabaF_1053474', 'JansJ_Ligav_1053661', 'NiedAn_Siksp_771071', 'KaijI_Sfink_886320', 'DeglA_Zelte_771027', 'VecoJ_ViktI_1296782', 'KaijI_Juga_886318', 'GulbA_Zigf_1046831', 'LapiK_NemiP_1293516', 'StraK_Kars_1051768', 'GulbA_Zigfr_1049519', 'DeglA_Patri_103063'}\n"
     ]
    }
   ],
   "source": [
    "# let's compare set of file stems in our anaylsis folder with set of stems in our parquet file\n",
    "# we will use set comprehension\n",
    "# we will use set comprehension\n",
    "\n",
    "# list of files in our target folder\n",
    "air_analysis_files = list(target_folder.iterdir())\n",
    "# set of stems in our target folder\n",
    "air_analysis_stems = {file.stem for file in air_analysis_files}\n",
    "# set of stems in our parquet file\n",
    "parquet_stems = set(df[\"file_stem\"].unique())\n",
    "# how many total of each?\n",
    "print(f\"Total stems in air analysis folder: {len(air_analysis_stems)}\")\n",
    "print(f\"Total stems in parquet file: {len(parquet_stems)}\")\n",
    "# let's see the difference\n",
    "print(f\"Stems in air analysis folder but not in parquet file: {air_analysis_stems - parquet_stems}\")\n",
    "print(f\"Stems in parquet file but not in air analysis folder: {parquet_stems - air_analysis_stems}\")\n",
    "\n",
    "# we can see that there have been some changes in the file names but it should not meaningfully affect our analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/analysis_air/2025_02_28_17_50_54')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.errors import EmptyDataError # we need to import this error\n",
    "# now let's create a function that takes a subfolder with responses and returns a dataframe with unique terms and their combined counts\n",
    "# to do so we will go through all csv files in subfolder\n",
    "# we will extract rows with unique terms and their counts\n",
    "# and then we will merge them into a single dataframe by adding counts\n",
    "# we will return this dataframe\n",
    "def get_combined_response_df(subfolder, verbose=False):\n",
    "    dfs = []\n",
    "    for file in tqdm(subfolder.iterdir()):\n",
    "        if file.suffix == \".csv\":\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "            except EmptyDataError:\n",
    "                print(f\"Empty data error for {file}\")\n",
    "                continue\n",
    "            # drop duplicates\n",
    "            df = df.drop_duplicates()\n",
    "            # name first column term\n",
    "            df.columns = [\"term\", \"term_freq\"]\n",
    "            # we also want to add doc_freq column with value 1\n",
    "            df[\"doc_freq\"] = 1\n",
    "            # df = df.groupby(\"term\").sum().reset_index()\n",
    "            dfs.append(df)\n",
    "    if verbose:\n",
    "        # print shape of first 5 dataframes\n",
    "        for df in dfs[:5]:\n",
    "            print(df.shape)\n",
    "    df = pd.concat(dfs).groupby(\"term\").sum().reset_index()\n",
    "    # sort alphabetically\n",
    "    # df = df.sort_values(\"term_freq\", ascending=False)\n",
    "    df = df.sort_values(\"doc_freq\", ascending=False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "458it [00:00, 948.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1744, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>vilciens</td>\n",
       "      <td>566</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>laivas</td>\n",
       "      <td>925</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>auto</td>\n",
       "      <td>1875</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>kamanas</td>\n",
       "      <td>149</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>rati</td>\n",
       "      <td>2222</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>kuģis</td>\n",
       "      <td>480</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>laivu</td>\n",
       "      <td>502</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>lidmašīna</td>\n",
       "      <td>546</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>kuģi</td>\n",
       "      <td>1513</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>tvaikonis</td>\n",
       "      <td>158</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term  term_freq  doc_freq\n",
       "1504   vilciens        566       102\n",
       "771      laivas        925        95\n",
       "160        auto       1875        83\n",
       "582     kamanas        149        71\n",
       "1176       rati       2222        63\n",
       "727       kuģis        480        60\n",
       "786       laivu        502        56\n",
       "825   lidmašīna        546        52\n",
       "722        kuģi       1513        52\n",
       "1386  tvaikonis        158        50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test it on first air folder\n",
    "air_df = get_combined_response_df(target_folder)\n",
    "# shape\n",
    "print(f\"Shape: {air_df.shape}\")\n",
    "#  head 10\n",
    "air_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save this dataframe to a csv file in parent of target folder\n",
    "# the file name will be air_combined with datetime stamp and then .csv\n",
    "# we will save it without index\n",
    "\n",
    "# target file\n",
    "target_file = Path(target_folder).parent / f\"air_combined_{now.strftime('%Y_%m_%d_%H_%M_%S')}.csv\"\n",
    "# save\n",
    "air_df.to_csv(target_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting tf and df for maritime responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maritime folders:\n",
      "..\\data\\analysis_maritime\\2025_02_27_21_05_04\n"
     ]
    }
   ],
   "source": [
    "# let's do the same for maritime folders\n",
    "maritime_target_parent = Path(\"../data\") / \"analysis_maritime\" \n",
    "# get subfolders\n",
    "maritime_folders = [f for f in maritime_target_parent.iterdir() if f.is_dir()]\n",
    "# print\n",
    "print(f\"Maritime folders:\")\n",
    "for folder in maritime_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/analysis_maritime/2025_02_27_21_05_04')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "maritime_target_folder = Path(maritime_folders[0])\n",
    "maritime_target_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "604it [00:00, 1956.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty data error for ..\\data\\analysis_maritime\\2025_02_27_21_05_04\\RozeL_StipK_964055.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "916it [00:01, 498.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 3)\n",
      "(5, 3)\n",
      "(7, 3)\n",
      "(14, 3)\n",
      "(2, 3)\n",
      "Shape: (1240, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>laivas</td>\n",
       "      <td>1146</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>kuģis</td>\n",
       "      <td>770</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>kuģi</td>\n",
       "      <td>2492</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>rati</td>\n",
       "      <td>2821</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>laivu</td>\n",
       "      <td>590</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>laivā</td>\n",
       "      <td>521</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>tvaikonis</td>\n",
       "      <td>196</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>vilciens</td>\n",
       "      <td>362</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>kamanas</td>\n",
       "      <td>141</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>laiva</td>\n",
       "      <td>944</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term  term_freq  doc_freq\n",
       "560      laivas       1146       183\n",
       "492       kuģis        770       138\n",
       "485        kuģi       2492       127\n",
       "843        rati       2821        86\n",
       "580       laivu        590        78\n",
       "581       laivā        521        65\n",
       "1003  tvaikonis        196        64\n",
       "1118   vilciens        362        63\n",
       "397     kamanas        141        60\n",
       "558       laiva        944        50"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maritime_df = get_combined_response_df(maritime_target_folder, verbose=True)\n",
    "# shape\n",
    "print(f\"Shape: {maritime_df.shape}\")\n",
    "#  head 10\n",
    "maritime_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save results to maritime_target_parent\n",
    "# target file will have name maritime_term_tf_doc_tf with datetime stamp and then .csv\n",
    "# we will save it without index\n",
    "target_file = maritime_target_parent / f\"maritime_term_tf_doc_tf_{now.strftime('%Y_%m_%d_%H_%M_%S')}.csv\"\n",
    "# save\n",
    "maritime_df.to_csv(target_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing combined LLM results from air and maritime prompts\n",
    "\n",
    "Next we will load the saved CSV files and see if we can combine some terms to obtain candidates for time series analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n",
      "..\\data\\analysis_maritime\\maritime_term_tf_doc_tf_2025_02_28_17_50_54_gemini_flash_15.csv\n"
     ]
    }
   ],
   "source": [
    "# martime analysis folder\n",
    "maritime_src_folder = Path(\"../data/analysis_maritime\")\n",
    "# get all csv files\n",
    "maritime_files = list(Path(maritime_src_folder).glob(\"*.csv\"))\n",
    "# sort by date created\n",
    "maritime_files = sorted(maritime_files, key=lambda x: x.stat().st_ctime) # st_ctime is depreceated but still works for time being\n",
    "# how many files do we have?\n",
    "print(f\"Number of files: {len(list(maritime_files))}\")\n",
    "# print names\n",
    "for file in maritime_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1240, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>laivas</td>\n",
       "      <td>1146</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kuģis</td>\n",
       "      <td>770</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kuģi</td>\n",
       "      <td>2492</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rati</td>\n",
       "      <td>2821</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>laivu</td>\n",
       "      <td>590</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     term  term_freq  doc_freq\n",
       "0  laivas       1146       183\n",
       "1   kuģis        770       138\n",
       "2    kuģi       2492       127\n",
       "3    rati       2821        86\n",
       "4   laivu        590        78"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the latest file into df\n",
    "maritime_df = pd.read_csv(maritime_files[-1])\n",
    "# shape\n",
    "print(f\"Shape: {maritime_df.shape}\")\n",
    "# head\n",
    "maritime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms: 1240\n"
     ]
    }
   ],
   "source": [
    "# let's sort by term see if we can combine some terms\n",
    "maritime_df = maritime_df.sort_values(\"term\")\n",
    "# let's see if we can combine some terms\n",
    "# we will use fuzzy matching\n",
    "# we will use rapidfuzz library\n",
    "import rapidfuzz\n",
    "from rapidfuzz import process\n",
    "# let's see how many unique terms we have\n",
    "unique_terms = maritime_df[\"term\"].unique()\n",
    "print(f\"Number of unique terms: {len(unique_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: \"Ausekli\"\n",
      "('\"Ausekli\"', 100.0, 0)\n"
     ]
    }
   ],
   "source": [
    "# we would like to combine terms that are similar\n",
    "# we will use process.extract to get similar terms\n",
    "# we will use threshold of 90\n",
    "# we will use limit of 5\n",
    "# we will use scorer fuzz.ratio\n",
    "# we will use rapidfuzz.fuzz.ratio\n",
    "# we will use rapidfuzz.fuzz.token_sort_ratio\n",
    "# we will use rapidfuzz.fuzz.token_set_ratio\n",
    "\n",
    "# let's test it on first term\n",
    "term = unique_terms[0]\n",
    "# get similar terms\n",
    "similar_terms = process.extract(term, unique_terms, scorer=rapidfuzz.fuzz.ratio, \n",
    "                                limit=10, \n",
    "                                score_cutoff=90)\n",
    "                                \n",
    "# print\n",
    "print(f\"Term: {term}\")\n",
    "for similar_term in similar_terms:\n",
    "    print(similar_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: kuģis\n",
      "('kuģis', 100.0, 492)\n",
      "('kuģits', 90.9090909090909, 499)\n",
      "('kušģis', 90.9090909090909, 545)\n",
      "('kuģi', 88.88888888888889, 485)\n",
      "('kuģelis', 83.33333333333334, 481)\n",
      "('kuģitis', 83.33333333333334, 498)\n",
      "('kuģītis', 83.33333333333334, 522)\n",
      "('kuģim', 80.0, 487)\n",
      "('kuģit', 80.0, 496)\n",
      "('kuģos', 80.0, 510)\n",
      "('kuģus', 80.0, 515)\n",
      "('kuģģi', 80.0, 516)\n",
      "('kuģši', 80.0, 530)\n",
      "('kuģģitis', 76.92307692307692, 517)\n",
      "('* kuģi', 72.72727272727273, 24)\n",
      "('kuģcus', 72.72727272727273, 479)\n",
      "('kuģeli', 72.72727272727273, 480)\n",
      "('kuģiem', 72.72727272727273, 486)\n",
      "('kuģiti', 72.72727272727273, 497)\n",
      "('kuģitī', 72.72727272727273, 500)\n",
      "('kuģiša', 72.72727272727273, 501)\n",
      "('kuģiši', 72.72727272727273, 502)\n",
      "('kuģišu', 72.72727272727273, 503)\n",
      "('kuģīti', 72.72727272727273, 520)\n",
      "('kuģīši', 72.72727272727273, 525)\n",
      "('bruņkuģis', 71.42857142857143, 152)\n",
      "('kaŗakuģis', 71.42857142857143, 432)\n",
      "('kuģinieks', 71.42857142857143, 490)\n",
      "('tankkuģis', 71.42857142857143, 938)\n",
      "('gaisakuģis', 66.66666666666667, 315)\n",
      "('kumēlis', 66.66666666666667, 461)\n",
      "('kuteris', 66.66666666666667, 469)\n",
      "('kučiers', 66.66666666666667, 474)\n",
      "('kuģa', 66.66666666666667, 475)\n",
      "('kuģniek', 66.66666666666667, 509)\n",
      "('kuģu', 66.66666666666667, 512)\n",
      "('kuģģiši', 66.66666666666667, 518)\n",
      "('kuģī', 66.66666666666667, 519)\n",
      "('kuģītim', 66.66666666666667, 521)\n",
      "('kuģīšos', 66.66666666666667, 527)\n",
      "('kuģīšus', 66.66666666666667, 529)\n",
      "('kuŗi', 66.66666666666667, 540)\n",
      "('kuši', 66.66666666666667, 543)\n",
      "('līnijkuģis', 66.66666666666667, 655)\n",
      "('mākoņkuģis', 66.66666666666667, 702)\n",
      "('zēģelkuģis', 66.66666666666667, 1189)\n",
      "('kuģneecibas', 62.5, 504)\n",
      "('kuģniecibas', 62.5, 506)\n",
      "('kuģniecības', 62.5, 507)\n",
      "('preču kuģis', 62.5, 810)\n"
     ]
    }
   ],
   "source": [
    "# how about kuģis?\n",
    "term = \"kuģis\"\n",
    "# get similar terms\n",
    "similar_terms = process.extract(term, unique_terms, scorer=rapidfuzz.fuzz.ratio, \n",
    "                                limit=50, \n",
    "                                score_cutoff=50)\n",
    "# print\n",
    "print(f\"Term: {term}\")\n",
    "for similar_term in similar_terms:\n",
    "    print(similar_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: laiva\n",
      "('laiva', 100.0, 558)\n",
      "('laimva', 90.9090909090909, 551)\n",
      "('laivam', 90.9090909090909, 559)\n",
      "('laivas', 90.9090909090909, 560)\n",
      "('laiv', 88.88888888888889, 557)\n",
      "('laiviņa', 83.33333333333334, 574)\n",
      "('laivāam', 83.33333333333334, 582)\n",
      "('laivu', 80.0, 580)\n",
      "('laivā', 80.0, 581)\n",
      "('lajva', 80.0, 586)\n",
      "('* laivas', 76.92307692307692, 25)\n",
      "('laiviņas', 76.92307692307692, 575)\n",
      "('lielaiva', 76.92307692307692, 610)\n",
      "('laipas', 72.72727272727273, 555)\n",
      "('laivām', 72.72727272727273, 583)\n",
      "('laivās', 72.72727272727273, 584)\n",
      "('burulaiva', 71.42857142857143, 190)\n",
      "('liellaiva', 71.42857142857143, 618)\n",
      "('zvejlaiva', 71.42857142857143, 1180)\n",
      "('Liellaivas', 66.66666666666667, 44)\n",
      "('burulaivas', 66.66666666666667, 191)\n",
      "('laivelē', 66.66666666666667, 562)\n",
      "('laivinieka', 66.66666666666667, 566)\n",
      "('laiviņu', 66.66666666666667, 576)\n",
      "('laiviņā', 66.66666666666667, 577)\n",
      "('lielalivas', 66.66666666666667, 611)\n",
      "('liellaivas', 66.66666666666667, 619)\n",
      "('lielļaivas', 66.66666666666667, 623)\n",
      "('motorlaiva', 66.66666666666667, 689)\n",
      "('zvejlaivas', 66.66666666666667, 1181)\n",
      "('airu laivas', 62.5, 64)\n",
      "('buru laivas', 62.5, 187)\n",
      "('kara laivas', 62.5, 409)\n",
      "('laiviniekam', 62.5, 567)\n",
      "('liel laivas', 62.5, 608)\n",
      "('motorlaivas', 62.5, 691)\n",
      "('tīklu laiva', 62.5, 1033)\n",
      "('upju laivas', 62.5, 1036)\n",
      "('laiveles', 61.53846153846154, 561)\n",
      "('laivinis', 61.53846153846154, 573)\n",
      "('laiviņām', 61.53846153846154, 578)\n",
      "('laiviņās', 61.53846153846154, 579)\n",
      "('Laivā', 60.0, 43)\n",
      "('laimā', 60.0, 552)\n",
      "('laimē', 60.0, 553)\n",
      "('laipu', 60.0, 556)\n",
      "('lajpa', 60.0, 585)\n",
      "('šaika', 60.0, 1214)\n",
      "('bagar-laivai', 58.82352941176471, 120)\n",
      "('buru laiviņa', 58.82352941176471, 188)\n"
     ]
    }
   ],
   "source": [
    "# let's try the same for laiva\n",
    "term = \"laiva\"\n",
    "# get similar terms\n",
    "similar_terms = process.extract(term, unique_terms, scorer=rapidfuzz.fuzz.ratio, \n",
    "                                limit=50, \n",
    "                                score_cutoff=50)\n",
    "# print\n",
    "print(f\"Term: {term}\")\n",
    "for similar_term in similar_terms:\n",
    "    print(similar_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see from above example a reasonable cutoff would be 70 that would still include various forms of the same word\n",
    "# also we might want to consider later filtering out terms that do not start with same letter as those might be compound words \n",
    "# we will want to treat compound words separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['deprel', 'form', 'index', 'lemma', 'parent', 'pos', 'tag', 'ufeats',\n",
       "       'upos', 'sent_ndx', 'author', 'title', 'dom_id', 'file_stem',\n",
       "       'file_stem_short', 'firstEdition', 'term'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing lemma set from big parquet data in latsenrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas: 777635\n",
      "Number of unique lemmas in lowercase: 731566\n"
     ]
    }
   ],
   "source": [
    "unique_lemma_set = set(df[\"lemma\"].unique())\n",
    "# how many unique lemmas do we have from large df?\n",
    "print(f\"Number of unique lemmas: {len(unique_lemma_set)}\")\n",
    "# this is much larger number than we would expect in Latvian language\n",
    "# mostly due to mispellings and ocr errors and other issues\n",
    "# let's convert this set to set of all lowercase lemmas \n",
    "unique_lemma_lower_set = {lemma.lower() for lemma in unique_lemma_set}\n",
    "# how many unique lemmas do we have in lowercase?\n",
    "print(f\"Number of unique lemmas in lowercase: {len(unique_lemma_lower_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma: kuģis vasara\n"
     ]
    }
   ],
   "source": [
    "# now let's remove all non-letter characters from lemmas,except for hyphens and regular whitespace\n",
    "# we will use regular expressions\n",
    "import re\n",
    "# let's test it on first lemma\n",
    "lemma = \"kuģis  ! *3 vasara\"\n",
    "# remove all non-letter characters\n",
    "lemma = re.sub(r\"[^a-zāčēģīķļņšūž -]\", \"\", lemma.lower())\n",
    "# we also want to replace occurrences of multiple spaces with single space\n",
    "lemma = re.sub(r\"\\s+\", \" \", lemma)\n",
    "# print\n",
    "print(f\"Lemma: {lemma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas in uncleaned set: 731566\n",
      "Number of unique lemmas in cleaned set: 701916\n"
     ]
    }
   ],
   "source": [
    "# now let's create a function that will take a lemma and return a cleaned lemma\n",
    "# we will remove all non-letter characters except hyphens and regular whitespace\n",
    "# we will also replace multiple spaces with single space\n",
    "def clean_lemma(lemma):\n",
    "    # lowercase\n",
    "    lemma = lemma.lower()\n",
    "    # replace ŗ with r\n",
    "    lemma = lemma.replace(\"ŗ\", \"r\")\n",
    "    # remove all non-letter characters\n",
    "    lemma = re.sub(r\"[^a-zāčēģīķļņšūž -]\", \"\", lemma.lower())\n",
    "    # replace multiple spaces with single space\n",
    "    lemma = re.sub(r\"\\s+\", \" \", lemma)\n",
    "    return lemma\n",
    "# now let's run it on set of unique unique_lemma_lower_set\n",
    "# how many unique lemmas do we have in uncleaned set?\n",
    "print(f\"Number of unique lemmas in uncleaned set: {len(unique_lemma_lower_set)}\")\n",
    "# let's create cleaned set\n",
    "cleaned_lemma_set = {clean_lemma(lemma) for lemma in unique_lemma_lower_set}\n",
    "# how many unique lemmas do we have in cleaned set?\n",
    "print(f\"Number of unique lemmas in cleaned set: {len(cleaned_lemma_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mīla: 7060\n",
      "ārprāts: 650\n",
      "vara: 7726\n",
      "romāns: 1028\n",
      "\": 836408\n",
      "redzēt: 58049\n",
      ",: 3041049\n",
      "vents: 669\n",
      "cik: 34624\n",
      "līksmi: 442\n"
     ]
    }
   ],
   "source": [
    "# since we have many erroronous lemma, idea is to filter out those lemma that occur less than some threshold\n",
    "# we will use threshold of 5 - assumption being that if lemma occurs less than 5 times it is likely an error\n",
    "# we will use collections.Counter to count occurrences of each lemma\n",
    "from collections import Counter\n",
    "# let's test it on first 10 lemmas\n",
    "lemma_counts = Counter(df[\"lemma\"].str.lower())\n",
    "# print first 10 lemma counts\n",
    "for lemma, count in list(lemma_counts.items())[:10]:\n",
    "    print(f\"{lemma}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lemma: 731566\n"
     ]
    }
   ],
   "source": [
    "# how many lemma do we have in total?\n",
    "print(f\"Total number of lemma: {len(lemma_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lemma that occur 5 or more times: 121086\n"
     ]
    }
   ],
   "source": [
    "# how many lemma do we have that occur 5 or more times?\n",
    "threshold = 5\n",
    "# we will use dictionary comprehension\n",
    "# we will use key value pair if value is greater than or equal to threshold\n",
    "# we will use items method to get key value pairs\n",
    "# we will use dict method to convert key value pairs to dictionary\n",
    "lemma_counts_filtered = dict({key: value for key, value in lemma_counts.items() if value >= threshold}.items())\n",
    "# how many lemma do we have that occur 5 or more times?\n",
    "print(f\"Total number of lemma that occur {threshold} or more times: {len(lemma_counts_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",: 3041049\n",
      ".: 2176740\n",
      "un: 1088962\n",
      "būt: 920018\n",
      "\": 836408\n",
      "-: 711881\n",
      "tas: 503867\n",
      "viņš: 500675\n",
      "es: 358826\n",
      "kas: 318374\n"
     ]
    }
   ],
   "source": [
    "# let's ordre lemma_counts_filtered by value\n",
    "# we will use sorted function\n",
    "# we will use lambda function\n",
    "# we will use itemgetter\n",
    "from operator import itemgetter\n",
    "lemma_counts_sorted = dict(sorted(lemma_counts_filtered.items(), key=itemgetter(1), reverse=True))\n",
    "# let's print first 10\n",
    "for lemma, count in list(lemma_counts_sorted.items())[:10]:\n",
    "    print(f\"{lemma}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laupītajs: 5\n",
      "mārietiņ: 5\n",
      "vēps: 5\n",
      "uriels: 5\n",
      "landze: 5\n",
      "miroņrate: 5\n",
      "rījnieks: 5\n",
      "grāvele: 5\n",
      "bogurski: 5\n",
      "mche-che-che: 5\n"
     ]
    }
   ],
   "source": [
    "# how about last 10\n",
    "for lemma, count in list(lemma_counts_sorted.items())[-10:]:\n",
    "    print(f\"{lemma}: {count}\")\n",
    "# we can see thatr some of the lemma are proper nouns but that is fine\n",
    "# we've made a reduction in lemma count by about 600% which is good\n",
    "# the current count of 120k lemma is still too high but we will leave it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas in cleaned set: 121086\n"
     ]
    }
   ],
   "source": [
    "# so cleaned_lemma_set will be keys of lemma_counts_filtered in a set\n",
    "cleaned_lemma_set = set(lemma_counts_filtered.keys())\n",
    "# how many unique lemmas do we have in cleaned_lemma_set?\n",
    "print(f\"Number of unique lemmas in cleaned set: {len(cleaned_lemma_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atsacitot\n",
      "kāsa\n",
      "lesim\n",
      "biljards\n",
      "apspis\n",
      "būiu\n",
      "tirpics\n",
      "fpēdams\n",
      "kuraã\n",
      "bizons\n",
      "logeem\n",
      "arbuzs\n",
      "priecadams\n",
      "andreas\n",
      "pamudinats\n",
      "nokļūsēt\n",
      "atslaucīt\n",
      "turpat\n",
      "šķeltnis\n",
      "fagrābt\n"
     ]
    }
   ],
   "source": [
    "# sample 10 random lemmas from cleaned set\n",
    "import random\n",
    "random_lemmas = random.sample(sorted(cleaned_lemma_set), 20)\n",
    "random.seed(2025)\n",
    "# print\n",
    "for lemma in random_lemmas:\n",
    "    print(lemma)\n",
    "# we can see that most are variations of misspellings and ocr errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"kuģis\" in cleaned_lemma_set # lookup should be in O(1) time for existance in set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms: 1240\n",
      "Number of unique terms in lowercase: 1237\n"
     ]
    }
   ],
   "source": [
    "# convert unique_terms to set\n",
    "unique_terms_set = set(unique_terms)\n",
    "# how many unique terms do we have?\n",
    "print(f\"Number of unique terms: {len(unique_terms_set)}\")\n",
    "# let's keep only lowercase terms\n",
    "unique_terms_set = {term.lower() for term in unique_terms_set}\n",
    "# how many unique terms do we have in lowercase?\n",
    "print(f\"Number of unique terms in lowercase: {len(unique_terms_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms: 1237\n",
      "Number of unique terms in cleaned set: 1227\n",
      "Difference between unique terms and cleaned terms: 82\n",
      "земūdens laivu\n",
      "\"krīvu\"\n",
      "ratí\n",
      "* baļķi\n",
      "«kurmis»\n",
      "\"ausekli\"\n",
      "kaŗakuģis\n",
      "kaŗakuģu\n",
      "куģiem\n",
      "* spārnu ratu\n"
     ]
    }
   ],
   "source": [
    "# now let's clean our unique terms using the same approach of lowercasing and removing non-letter characters\n",
    "# we will use the same function as well\n",
    "# how many unique terms do we have?\n",
    "print(f\"Number of unique terms: {len(unique_terms_set)}\")\n",
    "# let's create cleaned set\n",
    "cleaned_term_set = {clean_lemma(term) for term in unique_terms_set}\n",
    "# how many unique terms do we have in cleaned set?\n",
    "print(f\"Number of unique terms in cleaned set: {len(cleaned_term_set)}\")\n",
    "# what is difference between unique terms and cleaned terms?\n",
    "print(f\"Difference between unique terms and cleaned terms: {len(unique_terms_set - cleaned_term_set)}\")\n",
    "# let's get a sample of 10 terms that are in unique terms but not in cleaned terms\n",
    "random.seed(2025)\n",
    "sample_terms = random.sample(sorted(unique_terms_set - cleaned_term_set), 10)\n",
    "\n",
    "# print\n",
    "for term in sample_terms:\n",
    "    print(term)\n",
    "# we can see that our cleaning function got rid of some slavic characters and other non-letter characters\n",
    "# we also have replace ŗ with r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1237/1237 [00:00<00:00, 857437.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lemmas: 206\n",
      "aero\n",
      "aeroplana\n",
      "aeroplans\n",
      "aeroplāns\n",
      "aizjūgs\n",
      "arkls\n",
      "artilerija\n",
      "artilērija\n",
      "auto\n",
      "autobuss\n",
      "autokārs\n",
      "automašīna\n",
      "automobilis\n",
      "automobils\n",
      "automobīlis\n",
      "autotaksis\n",
      "bajars\n",
      "baltiņš\n",
      "barka\n",
      "barkase\n",
      "barkass\n",
      "barža\n",
      "berbep\n",
      "bobs\n",
      "brauceji\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# let's find only terms that are lemmas \n",
    "# to do so we will filter with a requirment that the term can be found in lemma column in the large df that we loaded earlier\n",
    "# it is not 100% accurate (as it is theoretically possible some lemma is not in the term) but it should be a good approximation\n",
    "\n",
    "\n",
    "\n",
    "lemmas = [term.lower() for term in tqdm(unique_terms_set) if term.lower() in cleaned_lemma_set]\n",
    "# how many lemmas do we have from our maritime terms?\n",
    "print(f\"Number of lemmas: {len(lemmas)}\")\n",
    "# first five sorted\n",
    "lemmas = sorted(lemmas)\n",
    "for lemma in lemmas[:25]:\n",
    "    print(lemma)\n",
    "# we see there are some lemma that are still a variation of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:00<00:00, 10798.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aero: ['aero']\n",
      "aeroplana: ['aeroplana', 'aeroplanu', 'aeroplani', 'aeroplanā', 'aeroplans', 'aeroplanis', 'aeroplāns']\n",
      "aeroplans: ['aeroplans', 'aeroplanis', 'aeroplanu', 'aeroplani', 'aeroplāns', 'aeroplana', 'aeroplanā']\n",
      "aeroplāns: ['aeroplāns', 'aeroplans', 'aeroplanis', 'aeroplanu', 'aeroplani', 'aeroplana', 'aeroplanā']\n",
      "aizjūgs: ['aizjūgs', 'aizjūgos', 'aizjūgus', 'aizjūgi', 'aizjūgā', 'aizjūga', 'aizjūgam', 'pajūgs']\n",
      "arkls: ['arkls']\n",
      "artilerija: ['artilerija', 'artilērija']\n",
      "artilērija: ['artilērija', 'artilerija']\n",
      "auto: ['auto', 'rato']\n",
      "autobuss: ['autobuss', 'autobusos', 'autobusā', 'autobusi', 'autobusa', 'autobusu']\n",
      "autokārs: ['autokārs']\n",
      "automašīna: ['automašīna', 'automašīnas', 'automašīnu', 'mašīna']\n",
      "automobilis: ['automobilis', 'automobils', 'automobili', 'automobilim', 'automobīlis', 'automobiļis', 'automobiļi', 'automobilī', 'automobiļos', 'automobiļus', 'automobiļiem', 'automobīlī', 'automobīļi', 'automobiļa', 'automobiļu']\n",
      "automobils: ['automobils', 'automobilis', 'automobili', 'automobilī', 'automobilim', 'automobīlis', 'automobiļos', 'automobiļus', 'automobiļis', 'automobīlī', 'automobiļi', 'automobīļi', 'automobiļa', 'automobiļu', 'automobiļam', 'automobīļos']\n",
      "automobīlis: ['automobīlis', 'automobilis', 'automobils', 'automobili', 'automobīlī', 'automobīļi', 'automobilim', 'automobiļos', 'automobiļus', 'automobiļis', 'automobīļos', 'automobīļiem', 'automobiļi', 'automobiļa', 'automobiļu', 'automobīļa', 'automobilī']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# let's create a dictionary with lemmas as keys and list of fuzzy similar terms as values\n",
    "# we will use a threshold of 75\n",
    "lemma_dict = {}\n",
    "for lemma in tqdm(lemmas):\n",
    "    similar_terms = process.extract(lemma, cleaned_term_set, scorer=rapidfuzz.fuzz.ratio, \n",
    "                                    limit=50, \n",
    "                                    score_cutoff=75)\n",
    "    lemma_dict[lemma] = [term for term, score, _ in similar_terms]\n",
    "\n",
    "# let's see first 5 keys and values\n",
    "for key, value in list(lemma_dict.items())[:15]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zirgs: ['zirgs', 'zirgi', 'zirgu', 'zirgelis']\n",
      "zirģelis: ['zirģelis', 'zirgelis']\n",
      "zvejlaiva: ['zvejlaiva', 'zvejlaivas', 'zvejas laivas', 'zvejnieku laiva']\n",
      "zārks: ['zārks']\n",
      "zēģele: ['zēģele', 'zēģeles', 'zēģelnieka']\n",
      "zēģelkuģis: ['zēģelkuģis', 'zēģelkuģišiem', 'zēģelniekus']\n",
      "ātrvilciens: ['ātrvilciens', 'ātrvilcienu', 'ātrvilcienā', 'vilciens', 'vilcienus', 'vilcienos', ' vilciens', 'sanitārvilciens', 'bruņuvilciens']\n",
      "četrjūgs: ['četrjūgs', 'četrjūgā', 'trijjūgs']\n",
      "čoliņa: ['čoliņa', 'čoliņu', 'čoliņā']\n",
      "ērzelis: ['ērzelis']\n",
      "ķēve: ['ķēve', 'ķēvi']\n",
      "šalons: ['šalons', 'ešalons', 'ešalonu', 'ešelons', 'ešaloni', 'galeons']\n",
      "škipers: ['škipers']\n",
      "šoneris: ['šoneris', 'šoneri', 'šonerus', 'šoneriem']\n",
      "štīmers: ['štīmers', 'štīmeri', 'stīmeris']\n"
     ]
    }
   ],
   "source": [
    "# how about last 5 items\n",
    "for key, value in list(lemma_dict.items())[-15:]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aero: ['aero']\n",
      "aeroplana: ['aeroplana', 'aeroplanu', 'aeroplani', 'aeroplanā', 'aeroplans', 'aeroplanis', 'aeroplāns']\n",
      "aeroplans: ['aeroplans', 'aeroplanis', 'aeroplanu', 'aeroplani', 'aeroplāns', 'aeroplana', 'aeroplanā']\n",
      "aeroplāns: ['aeroplāns', 'aeroplans', 'aeroplanis', 'aeroplanu', 'aeroplani', 'aeroplana', 'aeroplanā']\n",
      "aizjūgs: ['aizjūgs', 'aizjūgos', 'aizjūgus', 'aizjūgi', 'aizjūgā', 'aizjūga', 'aizjūgam']\n",
      "arkls: ['arkls']\n",
      "artilerija: ['artilerija', 'artilērija']\n",
      "artilērija: ['artilērija', 'artilerija']\n",
      "auto: ['auto']\n",
      "autobuss: ['autobuss', 'autobusos', 'autobusā', 'autobusi', 'autobusa', 'autobusu']\n",
      "autokārs: ['autokārs']\n",
      "automašīna: ['automašīna', 'automašīnas', 'automašīnu']\n",
      "automobilis: ['automobilis', 'automobils', 'automobili', 'automobilim', 'automobīlis', 'automobiļis', 'automobiļi', 'automobilī', 'automobiļos', 'automobiļus', 'automobiļiem', 'automobīlī', 'automobīļi', 'automobiļa', 'automobiļu']\n",
      "automobils: ['automobils', 'automobilis', 'automobili', 'automobilī', 'automobilim', 'automobīlis', 'automobiļos', 'automobiļus', 'automobiļis', 'automobīlī', 'automobiļi', 'automobīļi', 'automobiļa', 'automobiļu', 'automobiļam', 'automobīļos']\n",
      "automobīlis: ['automobīlis', 'automobilis', 'automobils', 'automobili', 'automobīlī', 'automobīļi', 'automobilim', 'automobiļos', 'automobiļus', 'automobiļis', 'automobīļos', 'automobīļiem', 'automobiļi', 'automobiļa', 'automobiļu', 'automobīļa', 'automobilī']\n"
     ]
    }
   ],
   "source": [
    "# let's filter the values in our lemma_dict so that only terms that start with same letter as lemma are included\n",
    "# also we want to exclude terms that have difference in length from original lemma by more than two characters\n",
    "# we will create a new dictionary with these requirements\n",
    "filtered_lemma_dict = {}\n",
    "for key, value in lemma_dict.items():\n",
    "    filtered_value = [term for term in value if term.startswith(key[0]) and abs(len(term) - len(key)) <= 2]\n",
    "    filtered_lemma_dict[key] = filtered_value\n",
    "\n",
    "# let's see first 5 keys and values\n",
    "for key, value in list(filtered_lemma_dict.items())[:15]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vērsis: ['vērsis']\n",
      "vēzums: ['vēzums', 'vēzumus', 'vezums', 'vezumos']\n",
      "zemūdene: ['zemūdene', 'zemūdenes', 'zemūdenē', 'zemūdeņu']\n",
      "zemūdenslaiva: ['zemūdenslaiva', 'zemūdenslaivas', 'zemūdens laivas', 'zemūdenslaivu', 'zemūdens laivu']\n",
      "zirdziņš: ['zirdziņš', 'zirdziņi']\n",
      "zirgs: ['zirgs', 'zirgi', 'zirgu']\n",
      "zirģelis: ['zirģelis', 'zirgelis']\n",
      "zārks: ['zārks']\n",
      "zēģele: ['zēģele', 'zēģeles']\n",
      "ātrvilciens: ['ātrvilciens', 'ātrvilcienu', 'ātrvilcienā']\n",
      "četrjūgs: ['četrjūgs', 'četrjūgā']\n",
      "čoliņa: ['čoliņa', 'čoliņu', 'čoliņā']\n",
      "ērzelis: ['ērzelis']\n",
      "ķēve: ['ķēve', 'ķēvi']\n",
      "šoneris: ['šoneris', 'šoneri', 'šonerus', 'šoneriem']\n"
     ]
    }
   ],
   "source": [
    "# last 15 items\n",
    "for key, value in list(filtered_lemma_dict.items())[-15:]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1227, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>laivas</th>\n",
       "      <td>1146</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuģis</th>\n",
       "      <td>770</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuģi</th>\n",
       "      <td>2492</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rati</th>\n",
       "      <td>2821</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laivu</th>\n",
       "      <td>590</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term_freq  doc_freq\n",
       "term                       \n",
       "laivas       1146       183\n",
       "kuģis         770       138\n",
       "kuģi         2492       127\n",
       "rati         2821        86\n",
       "laivu         590        78"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape\n",
    "print(f\"Shape: {maritime_df.shape}\")\n",
    "maritime_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate index values: 0\n"
     ]
    }
   ],
   "source": [
    "# let's create index for maritime_df that is term lowercased and contains only letters and hyphens\n",
    "# reset index first\n",
    "maritime_df = maritime_df.reset_index()\n",
    "maritime_df[\"term\"] = maritime_df[\"term\"].apply(clean_lemma)\n",
    "# use term as index\n",
    "maritime_df = maritime_df.set_index(\"term\")\n",
    "# let's see first 5 index values\n",
    "maritime_df.head()\n",
    "# do we have any duplicate index values?\n",
    "print(f\"Number of duplicate index values: {maritime_df.index.duplicated().sum()}\")\n",
    "# let's see the duplicates\n",
    "duplicates = maritime_df.index[maritime_df.index.duplicated()]\n",
    "# print\n",
    "for duplicate in duplicates:\n",
    "    print(duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baļķi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>divrati</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kuģi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>laivas</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       term  term_freq  doc_freq\n",
       "0                    0         7\n",
       "1     baļķi          0         1\n",
       "2   divrati          0         1\n",
       "3      kuģi          0         1\n",
       "4    laivas          0         2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's combine the rows with same index by summing term_freq and doc_freq\n",
    "# to do so we will first reset index\n",
    "maritime_df = maritime_df.reset_index()\n",
    "# now we will group by term and sum\n",
    "maritime_df = maritime_df.groupby(\"term\").sum().reset_index()\n",
    "# let's see first 5 rows\n",
    "maritime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1227, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>laivas</th>\n",
       "      <td>1146</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuģis</th>\n",
       "      <td>770</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuģi</th>\n",
       "      <td>2492</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rati</th>\n",
       "      <td>2821</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laivu</th>\n",
       "      <td>590</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laivā</th>\n",
       "      <td>522</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvaikonis</th>\n",
       "      <td>196</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vilciens</th>\n",
       "      <td>362</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kamanas</th>\n",
       "      <td>141</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laiva</th>\n",
       "      <td>944</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term_freq  doc_freq\n",
       "term                          \n",
       "laivas          1146       183\n",
       "kuģis            770       138\n",
       "kuģi            2492       127\n",
       "rati            2821        86\n",
       "laivu            590        78\n",
       "laivā            522        66\n",
       "tvaikonis        196        64\n",
       "vilciens         362        63\n",
       "kamanas          141        60\n",
       "laiva            944        50"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape\n",
    "print(f\"Shape: {maritime_df.shape}\")\n",
    "# sort by most doc_freq\n",
    "maritime_df = maritime_df.sort_values(\"doc_freq\", ascending=False)\n",
    "# let's see first 10 rows  \n",
    "maritime_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>laivas</th>\n",
       "      <td>1146</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuģis</th>\n",
       "      <td>770</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuģi</th>\n",
       "      <td>2492</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rati</th>\n",
       "      <td>2821</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laivu</th>\n",
       "      <td>590</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term_freq  doc_freq\n",
       "term                       \n",
       "laivas       1146       183\n",
       "kuģis         770       138\n",
       "kuģi         2492       127\n",
       "rati         2821        86\n",
       "laivu         590        78"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make term the index\n",
    "maritime_df = maritime_df.set_index(\"term\")\n",
    "# let's see first 5 index values\n",
    "maritime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:00<00:00, 32091.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>terms</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>laiv</td>\n",
       "      <td>[laiv, laivā, laiva, laivu, laivās, laivām, la...</td>\n",
       "      <td>3364</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>laiva</td>\n",
       "      <td>[laiva, laimva, laivas, laivam, laiv, laivāam,...</td>\n",
       "      <td>3283</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>kuģis</td>\n",
       "      <td>[kuģis, kuģits, kušģis, kuģi, kuģelis, kuģītis...</td>\n",
       "      <td>3410</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>laiviņa</td>\n",
       "      <td>[laiviņa, laiviņas, laiviņā, laiviņu, laiva, l...</td>\n",
       "      <td>2262</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>kuģa</td>\n",
       "      <td>[kuģa, kuģga, kuģiša, kuģīša, kuģu, kuģī, kuģi...</td>\n",
       "      <td>3547</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>kuģitis</td>\n",
       "      <td>[kuģitis, kuģģitis, kuģits, kuģiti, kuģītis, k...</td>\n",
       "      <td>825</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>kuģītis</td>\n",
       "      <td>[kuģītis, kuģīti, kuģītim, kuģitis, kuģis, kuģ...</td>\n",
       "      <td>818</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kuši</td>\n",
       "      <td>[kuši, kuģši, kugiši, kuģiši, kušģis, kuģīši, ...</td>\n",
       "      <td>2844</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>tvaikonis</td>\n",
       "      <td>[tvaikonis, tvaikoni, tvaikonītis, tvaikonitis...</td>\n",
       "      <td>422</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>kuģelis</td>\n",
       "      <td>[kuģelis, kuģeli, kuģis, kuģits, kušģis]</td>\n",
       "      <td>772</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>kuŗi</td>\n",
       "      <td>[kuri, kuģi, kuši]</td>\n",
       "      <td>2709</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>rats</td>\n",
       "      <td>[rats, ratus, ratos, rat, rata, rati, ratā, ra...</td>\n",
       "      <td>3034</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>tvaikonītis</td>\n",
       "      <td>[tvaikonītis, tvaikonīti, tvaikonitis, tvaikon...</td>\n",
       "      <td>243</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>vilciens</td>\n",
       "      <td>[vilciens, vilcienus, vilcienos, vilcieni, vil...</td>\n",
       "      <td>514</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>pajūgs</td>\n",
       "      <td>[pajūgs, pajūgus, pajšūgs, pajūgi, pajūgā, paj...</td>\n",
       "      <td>246</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>kamans</td>\n",
       "      <td>[kamans, kamanās, kamanas, kamanu, kamanām, ka...</td>\n",
       "      <td>301</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>tvaikonitis</td>\n",
       "      <td>[tvaikonitis, tvaikonītis, tvaikonis, tvaikoni...</td>\n",
       "      <td>231</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>automobilis</td>\n",
       "      <td>[automobilis, automobils, automobili, automobi...</td>\n",
       "      <td>195</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>ratiņš</td>\n",
       "      <td>[ratiņš, ratiņi, ratiņu, rati, ratiņus]</td>\n",
       "      <td>2842</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>automobīlis</td>\n",
       "      <td>[automobīlis, automobilis, automobils, automob...</td>\n",
       "      <td>184</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>kamanas</td>\n",
       "      <td>[kamanas, kamans, kamanās, kamanam, kamanu, ka...</td>\n",
       "      <td>252</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>automobils</td>\n",
       "      <td>[automobils, automobilis, automobili, automobi...</td>\n",
       "      <td>183</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>vezums</td>\n",
       "      <td>[vezums, vezumos, vezumais, vezumā, vezumi, vē...</td>\n",
       "      <td>356</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>plosts</td>\n",
       "      <td>[plosts, plostus, plosta, plosti, plostā, plostu]</td>\n",
       "      <td>286</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>kuša</td>\n",
       "      <td>[kuša, kuģiša, kuģīša, kuši, kuga, kušu, kuģa]</td>\n",
       "      <td>963</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>kuga</td>\n",
       "      <td>[kuga, kuģga, kuša, kuģa]</td>\n",
       "      <td>741</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>kuģga</td>\n",
       "      <td>[kuģga, kuga, kuģa, kuģgu]</td>\n",
       "      <td>662</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>lidmašīna</td>\n",
       "      <td>[lidmašīna, lidomašīna, lidmašīnas, lidmašīnai...</td>\n",
       "      <td>865</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>ragavas</td>\n",
       "      <td>[ragavas, ragavam, ragavās, ragas, ragavu]</td>\n",
       "      <td>127</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>jachta</td>\n",
       "      <td>[jachta, jachtas, jachtiņa, jachtu, jachtā, ja...</td>\n",
       "      <td>357</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>auto</td>\n",
       "      <td>[auto]</td>\n",
       "      <td>1121</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>lidmašina</td>\n",
       "      <td>[lidmašina, lidmašinam, lidmašinas, lidmašīna,...</td>\n",
       "      <td>842</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>vagons</td>\n",
       "      <td>[vagons, vagonos, vagonus, vagona, vagonu, vāg...</td>\n",
       "      <td>84</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>motorlaiva</td>\n",
       "      <td>[motorlaiva, motorlaivas, motorlaivu, motorlai...</td>\n",
       "      <td>144</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>važonis</td>\n",
       "      <td>[važonis, važoni, važons, vagons, važoņi, važo...</td>\n",
       "      <td>68</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>tramvajs</td>\n",
       "      <td>[tramvajs, tramvajos, tramvajā, tramvaji, tram...</td>\n",
       "      <td>52</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>lokomotive</td>\n",
       "      <td>[lokomotive, lokomotives, lokomotīve, lokomotī...</td>\n",
       "      <td>92</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>lokomotīve</td>\n",
       "      <td>[lokomotīve, lokomotīves, lokomotive, lokomoti...</td>\n",
       "      <td>79</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>lokomobīle</td>\n",
       "      <td>[lokomobīle, lokombīle, lokomobile, lokomobile...</td>\n",
       "      <td>75</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>ormanis</td>\n",
       "      <td>[ormanis, ormans, ormani, ormanim, ormaņi, orm...</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>liellaiva</td>\n",
       "      <td>[liellaiva, liellaivas, lielaiva, liel laivas,...</td>\n",
       "      <td>64</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>ormans</td>\n",
       "      <td>[ormans, ormanis, ormani, ormanī, oras, ormanim]</td>\n",
       "      <td>65</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>ratus</td>\n",
       "      <td>[ratus, rats, ratu, ratošus, ratoņus, rateļus,...</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>plostiņš</td>\n",
       "      <td>[plostiņš, plostiņi, plostiņu, plosti, plostiem]</td>\n",
       "      <td>90</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>vāģis</td>\n",
       "      <td>[vāģis, vāģi, vāģišos, vāģos, vāģus]</td>\n",
       "      <td>92</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>lāviņa</td>\n",
       "      <td>[lāviņa, laiviņa]</td>\n",
       "      <td>54</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>velkonis</td>\n",
       "      <td>[velkonis, velkoni, velkonītis, velkonīši, vel...</td>\n",
       "      <td>47</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>barka</td>\n",
       "      <td>[barka, barkas, barkass, barkasi, barkasā, bar...</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>mašīna</td>\n",
       "      <td>[mašīna, mašīnas, mašina, mašīnu]</td>\n",
       "      <td>172</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>barkase</td>\n",
       "      <td>[barkase, barkas, barkass, barkasi, barkasā, b...</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lemma                                              terms  \\\n",
       "99          laiv  [laiv, laivā, laiva, laivu, laivās, laivām, la...   \n",
       "100        laiva  [laiva, laimva, laivas, laivam, laiv, laivāam,...   \n",
       "90         kuģis  [kuģis, kuģits, kušģis, kuģi, kuģelis, kuģītis...   \n",
       "102      laiviņa  [laiviņa, laiviņas, laiviņā, laiviņu, laiva, l...   \n",
       "86          kuģa  [kuģa, kuģga, kuģiša, kuģīša, kuģu, kuģī, kuģi...   \n",
       "91       kuģitis  [kuģitis, kuģģitis, kuģits, kuģiti, kuģītis, k...   \n",
       "93       kuģītis  [kuģītis, kuģīti, kuģītim, kuģitis, kuģis, kuģ...   \n",
       "98          kuši  [kuši, kuģši, kugiši, kuģiši, kušģis, kuģīši, ...   \n",
       "173    tvaikonis  [tvaikonis, tvaikoni, tvaikonītis, tvaikonitis...   \n",
       "87       kuģelis           [kuģelis, kuģeli, kuģis, kuģits, kušģis]   \n",
       "96          kuŗi                                 [kuri, kuģi, kuši]   \n",
       "152         rats  [rats, ratus, ratos, rat, rata, rati, ratā, ra...   \n",
       "175  tvaikonītis  [tvaikonītis, tvaikonīti, tvaikonitis, tvaikon...   \n",
       "187     vilciens  [vilciens, vilcienus, vilcienos, vilcieni, vil...   \n",
       "141       pajūgs  [pajūgs, pajūgus, pajšūgs, pajūgi, pajūgā, paj...   \n",
       "75        kamans  [kamans, kamanās, kamanas, kamanu, kamanām, ka...   \n",
       "174  tvaikonitis  [tvaikonitis, tvaikonītis, tvaikonis, tvaikoni...   \n",
       "12   automobilis  [automobilis, automobils, automobili, automobi...   \n",
       "151       ratiņš            [ratiņš, ratiņi, ratiņu, rati, ratiņus]   \n",
       "14   automobīlis  [automobīlis, automobilis, automobils, automob...   \n",
       "74       kamanas  [kamanas, kamans, kamanās, kamanam, kamanu, ka...   \n",
       "13    automobils  [automobils, automobilis, automobili, automobi...   \n",
       "184       vezums  [vezums, vezumos, vezumais, vezumā, vezumi, vē...   \n",
       "144       plosts  [plosts, plostus, plosta, plosti, plostā, plostu]   \n",
       "97          kuša     [kuša, kuģiša, kuģīša, kuši, kuga, kušu, kuģa]   \n",
       "80          kuga                          [kuga, kuģga, kuša, kuģa]   \n",
       "88         kuģga                         [kuģga, kuga, kuģa, kuģgu]   \n",
       "105    lidmašīna  [lidmašīna, lidomašīna, lidmašīnas, lidmašīnai...   \n",
       "149      ragavas         [ragavas, ragavam, ragavās, ragas, ragavu]   \n",
       "71        jachta  [jachta, jachtas, jachtiņa, jachtu, jachtā, ja...   \n",
       "8           auto                                             [auto]   \n",
       "104    lidmašina  [lidmašina, lidmašinam, lidmašinas, lidmašīna,...   \n",
       "177       vagons  [vagons, vagonos, vagonus, vagona, vagonu, vāg...   \n",
       "130   motorlaiva  [motorlaiva, motorlaivas, motorlaivu, motorlai...   \n",
       "178      važonis  [važonis, važoni, važons, vagons, važoņi, važo...   \n",
       "170     tramvajs  [tramvajs, tramvajos, tramvajā, tramvaji, tram...   \n",
       "116   lokomotive  [lokomotive, lokomotives, lokomotīve, lokomotī...   \n",
       "117   lokomotīve  [lokomotīve, lokomotīves, lokomotive, lokomoti...   \n",
       "115   lokomobīle  [lokomobīle, lokombīle, lokomobile, lokomobile...   \n",
       "138      ormanis  [ormanis, ormans, ormani, ormanim, ormaņi, orm...   \n",
       "109    liellaiva  [liellaiva, liellaivas, lielaiva, liel laivas,...   \n",
       "139       ormans   [ormans, ormanis, ormani, ormanī, oras, ormanim]   \n",
       "153        ratus  [ratus, rats, ratu, ratošus, ratoņus, rateļus,...   \n",
       "143     plostiņš   [plostiņš, plostiņi, plostiņu, plosti, plostiem]   \n",
       "189        vāģis               [vāģis, vāģi, vāģišos, vāģos, vāģus]   \n",
       "119       lāviņa                                  [lāviņa, laiviņa]   \n",
       "180     velkonis  [velkonis, velkoni, velkonītis, velkonīši, vel...   \n",
       "18         barka  [barka, barkas, barkass, barkasi, barkasā, bar...   \n",
       "125       mašīna                  [mašīna, mašīnas, mašina, mašīnu]   \n",
       "19       barkase  [barkase, barkas, barkass, barkasi, barkasā, b...   \n",
       "\n",
       "     term_freq  doc_freq  \n",
       "99        3364       418  \n",
       "100       3283       405  \n",
       "90        3410       318  \n",
       "102       2262       301  \n",
       "86        3547       228  \n",
       "91         825       176  \n",
       "93         818       175  \n",
       "98        2844       148  \n",
       "173        422       147  \n",
       "87         772       142  \n",
       "96        2709       130  \n",
       "152       3034       118  \n",
       "175        243       101  \n",
       "187        514       100  \n",
       "141        246        98  \n",
       "75         301        97  \n",
       "174        231        95  \n",
       "12         195        92  \n",
       "151       2842        92  \n",
       "14         184        88  \n",
       "74         252        88  \n",
       "13         183        86  \n",
       "184        356        81  \n",
       "144        286        72  \n",
       "97         963        51  \n",
       "80         741        47  \n",
       "88         662        46  \n",
       "105        865        45  \n",
       "149        127        44  \n",
       "71         357        44  \n",
       "8         1121        43  \n",
       "104        842        42  \n",
       "177         84        38  \n",
       "130        144        36  \n",
       "178         68        34  \n",
       "170         52        33  \n",
       "116         92        33  \n",
       "117         79        31  \n",
       "115         75        29  \n",
       "138         65        28  \n",
       "109         64        27  \n",
       "139         65        26  \n",
       "153        154        25  \n",
       "143         90        24  \n",
       "189         92        23  \n",
       "119         54        22  \n",
       "180         47        21  \n",
       "18          47        18  \n",
       "125        172        18  \n",
       "19          43        16  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's create a a new dataframe using filtered_lemma_dict as a guide\n",
    "# rows will be lemmas - keyis from filtered_lemma_dict\n",
    "# there will be following columns:\n",
    "# terms - values from the filtered_lemma_dict\n",
    "# term_freq - sum of term_freq for terms\n",
    "# doc_freq - sum of doc_freq for terms\n",
    "# we will sort by doc_freq\n",
    "\n",
    "# first let's create a list of dictionaries\n",
    "data = []\n",
    "for key, value in tqdm(filtered_lemma_dict.items()):\n",
    "    term_freq = 0\n",
    "    # we need to sum all term_freq for items in value\n",
    "    for term in value:\n",
    "        term_freq += maritime_df.loc[term, \"term_freq\"]\n",
    "    doc_freq = 0\n",
    "    # we need to sum all doc_freq for items in value\n",
    "    for term in value:\n",
    "        doc_freq += maritime_df.loc[term, \"doc_freq\"]\n",
    "    data.append({\"lemma\": key, \"terms\": value, \"term_freq\": term_freq, \"doc_freq\": doc_freq})\n",
    "# let's create a dataframe\n",
    "lemma_df = pd.DataFrame(data)\n",
    "# sort by doc_freq\n",
    "lemma_df = lemma_df.sort_values(\"doc_freq\", ascending=False)\n",
    "# let's see first 10 rows\n",
    "lemma_df.head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save this dataframe to maritime_target_parent\n",
    "# target file will have name maritime_lemma_term_tf_doc_tf with datetime stamp and then .csv\n",
    "# we will save it without index\n",
    "target_file = maritime_target_folder.parent / f\"maritime_lemma_term_tf_doc_tf_{now.strftime('%Y_%m_%d_%H_%M_%S')}.csv\"\n",
    "# save\n",
    "lemma_df.to_csv(target_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is kuģitī in lemma column: True\n"
     ]
    }
   ],
   "source": [
    "# let's double check is really \"kuģitī\" in df.lemma column\n",
    "needle = \"kuģitī\"\n",
    "\n",
    "print(f\"Is {needle} in lemma column: {needle in df['lemma'].values}\")\n",
    "# this means the lemma column has a lot of dubious entries meaning the lemmatizer did not work as well as we would have hoped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting actual maritime terms for further analysis\n",
    "\n",
    "We have to make a judgment call on what terms we want to analyze further.\n",
    "Our choice is to manually select terms from top 50 that are related to maritime industry.\n",
    "\n",
    "We will keep demuinative terms and remove terms that are not related to maritime industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laiv\n",
      "laiva\n",
      "kuģis\n",
      "laiviņa\n",
      "kuģa\n",
      "kuģitis\n",
      "kuģītis\n",
      "kuši\n",
      "tvaikonis\n",
      "kuģelis\n",
      "kuŗi\n",
      "rats\n",
      "tvaikonītis\n",
      "vilciens\n",
      "pajūgs\n",
      "kamans\n",
      "tvaikonitis\n",
      "automobilis\n",
      "ratiņš\n",
      "automobīlis\n",
      "kamanas\n",
      "automobils\n",
      "vezums\n",
      "plosts\n",
      "kuša\n",
      "kuga\n",
      "kuģga\n",
      "lidmašīna\n",
      "ragavas\n",
      "jachta\n",
      "auto\n",
      "lidmašina\n",
      "vagons\n",
      "motorlaiva\n",
      "važonis\n",
      "tramvajs\n",
      "lokomotive\n",
      "lokomotīve\n",
      "lokomobīle\n",
      "ormanis\n",
      "liellaiva\n",
      "ormans\n",
      "ratus\n",
      "plostiņš\n",
      "vāģis\n",
      "lāviņa\n",
      "velkonis\n",
      "barka\n",
      "mašīna\n",
      "barkase\n"
     ]
    }
   ],
   "source": [
    "# print top 50 terms from index\n",
    "for term in lemma_df.lemma[:50]:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # terms to keep\n",
    "maritime_terms_to_keep = [\"laiva\", \"laiviņa\", \"kuģis\", \"kuģītis\", \"jachta\", \"plosts\", \"motorlaiva\", \"liellaiva\", \"tvaikonis\", \"gondola\"]\n",
    "# # let's save these files into a csv file in parent of target folder\n",
    "# # the file name will be maritime_terms_to_keep with datetime stamp and then .csv\n",
    "# # we will save it without index\n",
    "# # target file\n",
    "target_file = maritime_target_folder.parent / f\"maritime_terms_to_keep_{now.strftime('%Y_%m_%d_%H_%M_%S')}.csv\"\n",
    "with open(target_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(maritime_terms_to_keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting actual air transport terms for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n",
      "..\\data\\analysis_air\\air_term_tf_doc_tf_2025_02_28_17_50_54_gemini_flash_15.csv\n"
     ]
    }
   ],
   "source": [
    "# airtime analysis folder\n",
    "air_target_parent = Path(\"../data/analysis_air\")\n",
    "# get list of csv files\n",
    "air_files = list(air_target_parent.glob(\"*.csv\"))\n",
    "# sort by date created\n",
    "air_files = sorted(air_files, key=lambda x: x.stat().st_ctime) # st_ctime is depreceated but still works for time being\n",
    "# how many files do we have?\n",
    "print(f\"Number of files: {len(list(air_files))}\")\n",
    "# print names\n",
    "for file in air_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1744, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vilciens</td>\n",
       "      <td>566</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>laivas</td>\n",
       "      <td>925</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto</td>\n",
       "      <td>1875</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kamanas</td>\n",
       "      <td>149</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rati</td>\n",
       "      <td>2222</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kuģis</td>\n",
       "      <td>480</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>laivu</td>\n",
       "      <td>502</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lidmašīna</td>\n",
       "      <td>546</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kuģi</td>\n",
       "      <td>1513</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tvaikonis</td>\n",
       "      <td>158</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pajūgs</td>\n",
       "      <td>121</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lidmašīnas</td>\n",
       "      <td>372</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>zirgs</td>\n",
       "      <td>423</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>laivā</td>\n",
       "      <td>462</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ragavas</td>\n",
       "      <td>92</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pajūgi</td>\n",
       "      <td>94</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>automobiļi</td>\n",
       "      <td>91</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>zirgi</td>\n",
       "      <td>738</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>automobilī</td>\n",
       "      <td>114</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>automobilis</td>\n",
       "      <td>90</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kamanās</td>\n",
       "      <td>127</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ormanis</td>\n",
       "      <td>92</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tvaikoņi</td>\n",
       "      <td>71</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>```</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>zirgu</td>\n",
       "      <td>1150</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term  term_freq  doc_freq\n",
       "0      vilciens        566       102\n",
       "1        laivas        925        95\n",
       "2          auto       1875        83\n",
       "3       kamanas        149        71\n",
       "4          rati       2222        63\n",
       "5         kuģis        480        60\n",
       "6         laivu        502        56\n",
       "7     lidmašīna        546        52\n",
       "8          kuģi       1513        52\n",
       "9     tvaikonis        158        50\n",
       "10       pajūgs        121        45\n",
       "11   lidmašīnas        372        43\n",
       "12        zirgs        423        41\n",
       "13        laivā        462        36\n",
       "14      ragavas         92        35\n",
       "15       pajūgi         94        33\n",
       "16   automobiļi         91        32\n",
       "17        zirgi        738        32\n",
       "18   automobilī        114        32\n",
       "19  automobilis         90        31\n",
       "20      kamanās        127        30\n",
       "21      ormanis         92        27\n",
       "22     tvaikoņi         71        26\n",
       "23          ```          0        24\n",
       "24        zirgu       1150        24"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the latest file into df\n",
    "air_df = pd.read_csv(air_files[-1])\n",
    "# shape\n",
    "print(f\"Shape: {air_df.shape}\")\n",
    "# head\n",
    "air_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned air terms: 1744\n"
     ]
    }
   ],
   "source": [
    "# cleaned air terms\n",
    "cleaned_air_terms_list = air_df[\"term\"].apply(clean_lemma).to_list()\n",
    "# how clean air terms do we have?\n",
    "print(f\"Number of cleaned air terms: {len(cleaned_air_terms_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lemmas: 269\n",
      "Shape: (269, 3)\n",
      "Number of unique lemmas: 269\n"
     ]
    }
   ],
   "source": [
    "# now we want to keep only lemma that are in cleaned_lemma_set\n",
    "# we will use isin method\n",
    "# we will use str.lower method\n",
    "air_lemmas = air_df[\"term\"].str.lower().isin(cleaned_lemma_set)\n",
    "# how many lemmas do we have?\n",
    "print(f\"Number of lemmas: {air_lemmas.sum()}\")\n",
    "# convert air_lemmas to list\n",
    "air_lemmas = air_df[air_lemmas]\n",
    "# shape\n",
    "print(f\"Shape: {air_lemmas.shape}\")\n",
    "# we just want term as set\n",
    "air_lemmas_set = set(air_lemmas[\"term\"])\n",
    "# how many unique lemmas do we have?\n",
    "print(f\"Number of unique lemmas: {len(air_lemmas_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms in cleaned air term set: 262\n"
     ]
    }
   ],
   "source": [
    "# now let's create cleaned air_term set using same approach as for maritime terms\n",
    "# we will use clean_lemma function\n",
    "# we will use set comprehension\n",
    "# we will use str.lower method\n",
    "\n",
    "cleaned_air_term_set = {clean_lemma(term) for term in air_lemmas_set}\n",
    "# how many unique terms do we have in cleaned air term set?\n",
    "print(f\"Number of unique terms in cleaned air term set: {len(cleaned_air_term_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set, list)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cleaned_air_term_set), type(cleaned_air_terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/262 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262/262 [00:00<00:00, 2220.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ešalona: ['ešalona', 'ešaloni', 'ešalons', 'ešalonu', 'ešalonus', 'šalons', 'ešaloniem']\n",
      "straujš: ['straujš']\n",
      "ekipāža: ['ekipāža', 'ekipāžai', 'ekipāžas', 'ekipāžā', 'ekipažas', 'ekipāžām', 'ekipāžās']\n",
      "federrati: ['federrati']\n",
      "dūkanis: ['dūkanis', 'dūkanais']\n",
      "šoneris: ['šoneris', 'šoneri']\n",
      "karuselis: ['karuselis', 'kamelis']\n",
      "sikspārnis: ['sikspārnis']\n",
      "salnis: ['salnis']\n",
      "puskariete: ['puskariete', 'puskariete', 'puskarietei', 'puskarieti', 'puskarieti', 'puskarietē', 'puskarīte', 'kariete', 'kariete', 'karietei', 'karietes', 'karietes']\n",
      "autobuss: ['autobuss', 'autobusos', 'autobus', 'autobusa', 'autobusi', 'autobusu', 'autobusā', 'autobusam', 'autobusiem']\n",
      "kūlejs: ['kūlejs', 'kūleju']\n",
      "groži: ['groži', 'grožu']\n",
      "lode: ['lode']\n",
      "ragaviņas: ['ragaviņas', 'ragaviņās', 'ragavas', 'ragavas', 'ragaviņām', 'ragutiņas', 'ragavās']\n",
      "Last 15 items\n",
      "motorlaiva: ['motorlaiva', 'motorlaivas', 'motorlaivu', 'motorlaivā', 'motorlaivā', 'motorlaiviņas', 'motorlaiva kaija', 'motora']\n",
      "telegrafeja: ['telegrafeja']\n",
      "melnis: ['melnis', 'melni', 'melniš', 'kamelis', 'smelijs']\n",
      "šķimelis: ['šķimelis']\n",
      "divritenis: ['divritenis', 'divriteni', 'ritenis', 'divriteņos']\n",
      "eizenbānis: ['eizenbānis']\n",
      "dumo: ['dumo']\n",
      "divjūgs: ['divjūgs', 'divjūgi', 'divjūgu', 'divjūgā']\n",
      "reņģuciems: ['reņģuciems']\n",
      "automašīna: ['automašīna', 'automašīnu', 'automašīnās', 'automašinu', 'automašinās', 'mašīna']\n",
      "vēzums: ['vēzums', 'vezums', 'vezumus']\n",
      "arkls: ['arkls', 'arkli', 'orkls']\n",
      "ormanis: ['ormanis', 'ormani', 'ormānis', 'fūrmanis', 'ormanī', 'ormaņi']\n",
      "tors: ['tors', 'ores', 'oris']\n",
      "vairogs: ['vairogs', 'varags']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now we want to create air_lemma_dict where keys are lemmas and values are similar terms\n",
    "# we will use the same approach as for maritime terms\n",
    "# we will use threshold of 75\n",
    "air_lemma_dict = {}\n",
    "for lemma in tqdm(cleaned_air_term_set):\n",
    "    similar_terms = process.extract(lemma, sorted(cleaned_air_terms_list), scorer=rapidfuzz.fuzz.ratio, \n",
    "                                    limit=50, \n",
    "                                    score_cutoff=75)\n",
    "    air_lemma_dict[lemma] = [term for term, score, _ in similar_terms]\n",
    "# let's see first 5 keys and values\n",
    "for key, value in list(air_lemma_dict.items())[:15]:\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"Last 15 items\")\n",
    "# last 15 items\n",
    "for key, value in list(air_lemma_dict.items())[-15:]:\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1744, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vilciens</th>\n",
       "      <td>566</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laivas</th>\n",
       "      <td>925</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto</th>\n",
       "      <td>1875</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kamanas</th>\n",
       "      <td>149</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rati</th>\n",
       "      <td>2222</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term_freq  doc_freq\n",
       "term                         \n",
       "vilciens        566       102\n",
       "laivas          925        95\n",
       "auto           1875        83\n",
       "kamanas         149        71\n",
       "rati           2222        63"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if term is not index make it index\n",
    "if \"term\" in air_df.columns:\n",
    "    air_df = air_df.set_index(\"term\")\n",
    "# air_df shape\n",
    "print(f\"Shape: {air_df.shape}\")\n",
    "# head()\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vilciens</th>\n",
       "      <td>566</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laivas</th>\n",
       "      <td>925</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto</th>\n",
       "      <td>1875</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kamanas</th>\n",
       "      <td>149</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rati</th>\n",
       "      <td>2222</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term_freq  doc_freq\n",
       "term                         \n",
       "vilciens        566       102\n",
       "laivas          925        95\n",
       "auto           1875        83\n",
       "kamanas         149        71\n",
       "rati           2222        63"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's normalize term\n",
    "# first reset index if term is index\n",
    "if \"term\" in air_df.index.name:\n",
    "    air_df = air_df.reset_index()\n",
    "# now let's normalize term\n",
    "air_df[\"term\"] = air_df[\"term\"].apply(clean_lemma)\n",
    "# make term the index\n",
    "air_df = air_df.set_index(\"term\")\n",
    "# let's see first 5 index values\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vilciens</th>\n",
       "      <td>566</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laivas</th>\n",
       "      <td>925</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto</th>\n",
       "      <td>1875</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kamanas</th>\n",
       "      <td>149</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rati</th>\n",
       "      <td>2222</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term_freq  doc_freq\n",
       "term                         \n",
       "vilciens        566       102\n",
       "laivas          925        95\n",
       "auto           1875        83\n",
       "kamanas         149        71\n",
       "rati           2222        63"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's get rid of duplicate rows\n",
    "# first reset index\n",
    "air_df = air_df.reset_index()\n",
    "# drop duplicates\n",
    "air_df = air_df.drop_duplicates()\n",
    "# make term the index\n",
    "air_df = air_df.set_index(\"term\")\n",
    "# let's see first 5 index values\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate index values: 30\n",
      "Number of duplicate index values: 0\n"
     ]
    }
   ],
   "source": [
    "# any duplicate index values?\n",
    "print(f\"Number of duplicate index values: {air_df.index.duplicated().sum()}\")\n",
    "# let's drop the duplicates\n",
    "air_df = air_df[~air_df.index.duplicated()]\n",
    "# any duplicate index values?\n",
    "print(f\"Number of duplicate index values: {air_df.index.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262/262 [00:00<00:00, 28320.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>terms</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>laiv</td>\n",
       "      <td>[laiv, laivā, laiva, laivu, laivās, laivām, la...</td>\n",
       "      <td>3364</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>laiva</td>\n",
       "      <td>[laiva, laimva, laivas, laivam, laiv, laivāam,...</td>\n",
       "      <td>3283</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>kuģis</td>\n",
       "      <td>[kuģis, kuģits, kušģis, kuģi, kuģelis, kuģītis...</td>\n",
       "      <td>3410</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>laiviņa</td>\n",
       "      <td>[laiviņa, laiviņas, laiviņā, laiviņu, laiva, l...</td>\n",
       "      <td>2262</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>kuģa</td>\n",
       "      <td>[kuģa, kuģga, kuģiša, kuģīša, kuģu, kuģī, kuģi...</td>\n",
       "      <td>3547</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>kuģitis</td>\n",
       "      <td>[kuģitis, kuģģitis, kuģits, kuģiti, kuģītis, k...</td>\n",
       "      <td>825</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>kuģītis</td>\n",
       "      <td>[kuģītis, kuģīti, kuģītim, kuģitis, kuģis, kuģ...</td>\n",
       "      <td>818</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kuši</td>\n",
       "      <td>[kuši, kuģši, kugiši, kuģiši, kušģis, kuģīši, ...</td>\n",
       "      <td>2844</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>tvaikonis</td>\n",
       "      <td>[tvaikonis, tvaikoni, tvaikonītis, tvaikonitis...</td>\n",
       "      <td>422</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>kuģelis</td>\n",
       "      <td>[kuģelis, kuģeli, kuģis, kuģits, kušģis]</td>\n",
       "      <td>772</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>kuŗi</td>\n",
       "      <td>[kuri, kuģi, kuši]</td>\n",
       "      <td>2709</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>rats</td>\n",
       "      <td>[rats, ratus, ratos, rat, rata, rati, ratā, ra...</td>\n",
       "      <td>3034</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>tvaikonītis</td>\n",
       "      <td>[tvaikonītis, tvaikonīti, tvaikonitis, tvaikon...</td>\n",
       "      <td>243</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>vilciens</td>\n",
       "      <td>[vilciens, vilcienus, vilcienos, vilcieni, vil...</td>\n",
       "      <td>514</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>pajūgs</td>\n",
       "      <td>[pajūgs, pajūgus, pajšūgs, pajūgi, pajūgā, paj...</td>\n",
       "      <td>246</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>kamans</td>\n",
       "      <td>[kamans, kamanās, kamanas, kamanu, kamanām, ka...</td>\n",
       "      <td>301</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>tvaikonitis</td>\n",
       "      <td>[tvaikonitis, tvaikonītis, tvaikonis, tvaikoni...</td>\n",
       "      <td>231</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>automobilis</td>\n",
       "      <td>[automobilis, automobils, automobili, automobi...</td>\n",
       "      <td>195</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>ratiņš</td>\n",
       "      <td>[ratiņš, ratiņi, ratiņu, rati, ratiņus]</td>\n",
       "      <td>2842</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>automobīlis</td>\n",
       "      <td>[automobīlis, automobilis, automobils, automob...</td>\n",
       "      <td>184</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>kamanas</td>\n",
       "      <td>[kamanas, kamans, kamanās, kamanam, kamanu, ka...</td>\n",
       "      <td>252</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>automobils</td>\n",
       "      <td>[automobils, automobilis, automobili, automobi...</td>\n",
       "      <td>183</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>vezums</td>\n",
       "      <td>[vezums, vezumos, vezumais, vezumā, vezumi, vē...</td>\n",
       "      <td>356</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>plosts</td>\n",
       "      <td>[plosts, plostus, plosta, plosti, plostā, plostu]</td>\n",
       "      <td>286</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>kuša</td>\n",
       "      <td>[kuša, kuģiša, kuģīša, kuši, kuga, kušu, kuģa]</td>\n",
       "      <td>963</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>kuga</td>\n",
       "      <td>[kuga, kuģga, kuša, kuģa]</td>\n",
       "      <td>741</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>kuģga</td>\n",
       "      <td>[kuģga, kuga, kuģa, kuģgu]</td>\n",
       "      <td>662</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>lidmašīna</td>\n",
       "      <td>[lidmašīna, lidomašīna, lidmašīnas, lidmašīnai...</td>\n",
       "      <td>865</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>ragavas</td>\n",
       "      <td>[ragavas, ragavam, ragavās, ragas, ragavu]</td>\n",
       "      <td>127</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>jachta</td>\n",
       "      <td>[jachta, jachtas, jachtiņa, jachtu, jachtā, ja...</td>\n",
       "      <td>357</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>auto</td>\n",
       "      <td>[auto]</td>\n",
       "      <td>1121</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>lidmašina</td>\n",
       "      <td>[lidmašina, lidmašinam, lidmašinas, lidmašīna,...</td>\n",
       "      <td>842</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>vagons</td>\n",
       "      <td>[vagons, vagonos, vagonus, vagona, vagonu, vāg...</td>\n",
       "      <td>84</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>motorlaiva</td>\n",
       "      <td>[motorlaiva, motorlaivas, motorlaivu, motorlai...</td>\n",
       "      <td>144</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>važonis</td>\n",
       "      <td>[važonis, važoni, važons, vagons, važoņi, važo...</td>\n",
       "      <td>68</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>tramvajs</td>\n",
       "      <td>[tramvajs, tramvajos, tramvajā, tramvaji, tram...</td>\n",
       "      <td>52</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>lokomotive</td>\n",
       "      <td>[lokomotive, lokomotives, lokomotīve, lokomotī...</td>\n",
       "      <td>92</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>lokomotīve</td>\n",
       "      <td>[lokomotīve, lokomotīves, lokomotive, lokomoti...</td>\n",
       "      <td>79</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>lokomobīle</td>\n",
       "      <td>[lokomobīle, lokombīle, lokomobile, lokomobile...</td>\n",
       "      <td>75</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>ormanis</td>\n",
       "      <td>[ormanis, ormans, ormani, ormanim, ormaņi, orm...</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>liellaiva</td>\n",
       "      <td>[liellaiva, liellaivas, lielaiva, liel laivas,...</td>\n",
       "      <td>64</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>ormans</td>\n",
       "      <td>[ormans, ormanis, ormani, ormanī, oras, ormanim]</td>\n",
       "      <td>65</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>ratus</td>\n",
       "      <td>[ratus, rats, ratu, ratošus, ratoņus, rateļus,...</td>\n",
       "      <td>154</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>plostiņš</td>\n",
       "      <td>[plostiņš, plostiņi, plostiņu, plosti, plostiem]</td>\n",
       "      <td>90</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>vāģis</td>\n",
       "      <td>[vāģis, vāģi, vāģišos, vāģos, vāģus]</td>\n",
       "      <td>92</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>lāviņa</td>\n",
       "      <td>[lāviņa, laiviņa]</td>\n",
       "      <td>54</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>velkonis</td>\n",
       "      <td>[velkonis, velkoni, velkonītis, velkonīši, vel...</td>\n",
       "      <td>47</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>barka</td>\n",
       "      <td>[barka, barkas, barkass, barkasi, barkasā, bar...</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>mašīna</td>\n",
       "      <td>[mašīna, mašīnas, mašina, mašīnu]</td>\n",
       "      <td>172</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>karite</td>\n",
       "      <td>[karite, kariete, karietes, karietē]</td>\n",
       "      <td>130</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lemma                                              terms  \\\n",
       "99          laiv  [laiv, laivā, laiva, laivu, laivās, laivām, la...   \n",
       "100        laiva  [laiva, laimva, laivas, laivam, laiv, laivāam,...   \n",
       "90         kuģis  [kuģis, kuģits, kušģis, kuģi, kuģelis, kuģītis...   \n",
       "102      laiviņa  [laiviņa, laiviņas, laiviņā, laiviņu, laiva, l...   \n",
       "86          kuģa  [kuģa, kuģga, kuģiša, kuģīša, kuģu, kuģī, kuģi...   \n",
       "91       kuģitis  [kuģitis, kuģģitis, kuģits, kuģiti, kuģītis, k...   \n",
       "93       kuģītis  [kuģītis, kuģīti, kuģītim, kuģitis, kuģis, kuģ...   \n",
       "98          kuši  [kuši, kuģši, kugiši, kuģiši, kušģis, kuģīši, ...   \n",
       "173    tvaikonis  [tvaikonis, tvaikoni, tvaikonītis, tvaikonitis...   \n",
       "87       kuģelis           [kuģelis, kuģeli, kuģis, kuģits, kušģis]   \n",
       "96          kuŗi                                 [kuri, kuģi, kuši]   \n",
       "152         rats  [rats, ratus, ratos, rat, rata, rati, ratā, ra...   \n",
       "175  tvaikonītis  [tvaikonītis, tvaikonīti, tvaikonitis, tvaikon...   \n",
       "187     vilciens  [vilciens, vilcienus, vilcienos, vilcieni, vil...   \n",
       "141       pajūgs  [pajūgs, pajūgus, pajšūgs, pajūgi, pajūgā, paj...   \n",
       "75        kamans  [kamans, kamanās, kamanas, kamanu, kamanām, ka...   \n",
       "174  tvaikonitis  [tvaikonitis, tvaikonītis, tvaikonis, tvaikoni...   \n",
       "12   automobilis  [automobilis, automobils, automobili, automobi...   \n",
       "151       ratiņš            [ratiņš, ratiņi, ratiņu, rati, ratiņus]   \n",
       "14   automobīlis  [automobīlis, automobilis, automobils, automob...   \n",
       "74       kamanas  [kamanas, kamans, kamanās, kamanam, kamanu, ka...   \n",
       "13    automobils  [automobils, automobilis, automobili, automobi...   \n",
       "184       vezums  [vezums, vezumos, vezumais, vezumā, vezumi, vē...   \n",
       "144       plosts  [plosts, plostus, plosta, plosti, plostā, plostu]   \n",
       "97          kuša     [kuša, kuģiša, kuģīša, kuši, kuga, kušu, kuģa]   \n",
       "80          kuga                          [kuga, kuģga, kuša, kuģa]   \n",
       "88         kuģga                         [kuģga, kuga, kuģa, kuģgu]   \n",
       "105    lidmašīna  [lidmašīna, lidomašīna, lidmašīnas, lidmašīnai...   \n",
       "149      ragavas         [ragavas, ragavam, ragavās, ragas, ragavu]   \n",
       "71        jachta  [jachta, jachtas, jachtiņa, jachtu, jachtā, ja...   \n",
       "8           auto                                             [auto]   \n",
       "104    lidmašina  [lidmašina, lidmašinam, lidmašinas, lidmašīna,...   \n",
       "177       vagons  [vagons, vagonos, vagonus, vagona, vagonu, vāg...   \n",
       "130   motorlaiva  [motorlaiva, motorlaivas, motorlaivu, motorlai...   \n",
       "178      važonis  [važonis, važoni, važons, vagons, važoņi, važo...   \n",
       "170     tramvajs  [tramvajs, tramvajos, tramvajā, tramvaji, tram...   \n",
       "116   lokomotive  [lokomotive, lokomotives, lokomotīve, lokomotī...   \n",
       "117   lokomotīve  [lokomotīve, lokomotīves, lokomotive, lokomoti...   \n",
       "115   lokomobīle  [lokomobīle, lokombīle, lokomobile, lokomobile...   \n",
       "138      ormanis  [ormanis, ormans, ormani, ormanim, ormaņi, orm...   \n",
       "109    liellaiva  [liellaiva, liellaivas, lielaiva, liel laivas,...   \n",
       "139       ormans   [ormans, ormanis, ormani, ormanī, oras, ormanim]   \n",
       "153        ratus  [ratus, rats, ratu, ratošus, ratoņus, rateļus,...   \n",
       "143     plostiņš   [plostiņš, plostiņi, plostiņu, plosti, plostiem]   \n",
       "189        vāģis               [vāģis, vāģi, vāģišos, vāģos, vāģus]   \n",
       "119       lāviņa                                  [lāviņa, laiviņa]   \n",
       "180     velkonis  [velkonis, velkoni, velkonītis, velkonīši, vel...   \n",
       "18         barka  [barka, barkas, barkass, barkasi, barkasā, bar...   \n",
       "125       mašīna                  [mašīna, mašīnas, mašina, mašīnu]   \n",
       "77        karite               [karite, kariete, karietes, karietē]   \n",
       "\n",
       "     term_freq  doc_freq  \n",
       "99        3364       418  \n",
       "100       3283       405  \n",
       "90        3410       318  \n",
       "102       2262       301  \n",
       "86        3547       228  \n",
       "91         825       176  \n",
       "93         818       175  \n",
       "98        2844       148  \n",
       "173        422       147  \n",
       "87         772       142  \n",
       "96        2709       130  \n",
       "152       3034       118  \n",
       "175        243       101  \n",
       "187        514       100  \n",
       "141        246        98  \n",
       "75         301        97  \n",
       "174        231        95  \n",
       "12         195        92  \n",
       "151       2842        92  \n",
       "14         184        88  \n",
       "74         252        88  \n",
       "13         183        86  \n",
       "184        356        81  \n",
       "144        286        72  \n",
       "97         963        51  \n",
       "80         741        47  \n",
       "88         662        46  \n",
       "105        865        45  \n",
       "149        127        44  \n",
       "71         357        44  \n",
       "8         1121        43  \n",
       "104        842        42  \n",
       "177         84        38  \n",
       "130        144        36  \n",
       "178         68        34  \n",
       "170         52        33  \n",
       "116         92        33  \n",
       "117         79        31  \n",
       "115         75        29  \n",
       "138         65        28  \n",
       "109         64        27  \n",
       "139         65        26  \n",
       "153        154        25  \n",
       "143         90        24  \n",
       "189         92        23  \n",
       "119         54        22  \n",
       "180         47        21  \n",
       "18          47        18  \n",
       "125        172        18  \n",
       "77         130        16  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's create a a new dataframe using air_lemma_dict as a guide\n",
    "# rows will be lemmas - keyis from air_lemma_dict\n",
    "# there will be following columns:\n",
    "# terms - values from the air_lemma_dict\n",
    "# term_freq - sum of term_freq for terms\n",
    "# doc_freq - sum of doc_freq for terms\n",
    "# we will sort by doc_freq\n",
    "\n",
    "# first let's create a list of dictionaries\n",
    "data = []\n",
    "for key, value in tqdm(air_lemma_dict.items()):\n",
    "    term_freq = 0\n",
    "    # we need to sum all term_freq for items in value\n",
    "    for term in value:\n",
    "        try:\n",
    "            term_freq += air_df.loc[term, \"term_freq\"]\n",
    "        except KeyError:\n",
    "            print(f\"term_freq KeyError for {term}\")\n",
    "    doc_freq = 0\n",
    "    # we need to sum all doc_freq for items in value\n",
    "    for term in value:\n",
    "        try:\n",
    "            doc_freq += air_df.loc[term, \"doc_freq\"]\n",
    "        except KeyError:\n",
    "            print(f\"doc_freq KeyError for {term}\")  \n",
    "    data.append({\"lemma\": key, \"terms\": value, \"term_freq\": term_freq, \"doc_freq\": doc_freq})\n",
    "# let's create a dataframe\n",
    "air_lemma_df = pd.DataFrame(data)\n",
    "# sort by doc_freq\n",
    "air_lemma_df = lemma_df.sort_values(\"doc_freq\", ascending=False)\n",
    "# let's see first 10 rows\n",
    "air_lemma_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save this dataframe to air_target_parent\n",
    "air_target_csv_file = air_target_parent / f\"air_lemma_term_tf_doc_tf_{now.strftime('%Y_%m_%d_%H_%M_%S')}.csv\"\n",
    "# save\n",
    "air_lemma_df.to_csv(air_target_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's  have a manual list of air terms from air_lemma_df\n",
    "# we have very few air terms so we can do it manually\n",
    "air_target_terms = [\"lidmašīna\", \"dirižablis\", \"cepelīns\", \"aeroplāns\", \"hidroplāns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save those terms to a csv file in parent of target folder\n",
    "# the file name will be air_target_terms with datetime stamp and then .csv\n",
    "# we will save it without index\n",
    "# target file\n",
    "target_file = air_target_parent / f\"air_target_terms_{now.strftime('%Y_%m_%d_%H_%M_%S')}.csv\"\n",
    "with open(target_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(air_target_terms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
